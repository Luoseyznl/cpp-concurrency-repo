1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
1.8.1
1.8.2
1.8.3
1.8.4
1.8.5
1.9
1.9.1
1.9.2
1.9.3
1.9.4
1.9.5
1.9.6
1.10
1.10.1
1.10.2
1.10.3
1.10.4
1.11
1.11.1
1.11.2
1.11.3
1.11.4
1.11.5
1.12
1.12.1
1.12.2
1.12.3
目录
Introduction
第一版的赞许
前言
感谢
关于本书
关于作者
关于封面
第1章 你好,并发世界
1.1 何谓并发
1.2 为什么使用并发
1.3 并发和多线程
1.4 开始入门
1.5 本章总结
第2章 线程管理
2.1 线程的基本操作
2.2 传递参数
2.3 转移所有权
2.4 确定线程数量
2.5 线程标识
2.6 本章总结
第3章 共享数据
3.1 共享数据的问题
3.2 使用互斥量
3.3 保护共享数据的方式
3.4 本章总结
第4章 同步操作
4.1 等待事件或条件
4.2 使用future
4.3 限时等待
4.4 简化代码
4.5 本章总结
第5章 内存模型和原子操作
5.1 内存模型
5.2 原子操作和原子类型
5.3 同步操作和强制排序


1.12.4
1.13
1.13.1
1.13.2
1.13.3
1.13.4
1.14
1.14.1
1.14.2
1.14.3
1.14.4
1.15
1.15.1
1.15.2
1.15.3
1.15.4
1.15.5
1.15.6
1.16
1.16.1
1.16.2
1.16.3
1.17
1.17.1
1.17.2
1.17.3
1.17.4
1.18
1.18.1
1.18.2
1.18.3
1.19
1.19.1
1.19.2
1.19.3
1.19.4
1.19.5
1.19.6
5.4 本章总结
第6章 设计基于锁的并发数据结构
6.1 并发设计的意义
6.2 基于锁的并发数据结构
6.3 设计更加复杂的数据结构
6.4 本章总结
第7章 设计无锁的并发数据结构
7.1 定义和意义
7.2 无锁数据结构的例子
7.3 设计无锁数据结构的指导建议
7.4 本章总结
第8章 并发设计
8.1 线程间划分工作
8.2 并发代码的性能
8.3 为多线程性能设计数据结构
8.4 设计并发代码的注意事项
8.5 在实践中设计并发代码
8.6 本章总结
第9章 高级线程管理
9.1 线程池
9.2 中断线程
9.3 本章总结
第10章 并行算法
10.1 并行化标准库算法
10.2 执行策略
10.3 C++标准库中的并行算法
10.4 本章总结
第11章 测试和调试多线程应用
11.1 与并发相关的Bug
11.2 定位并发Bug的技巧
11.3 本章总结
附录A C++11特性简明参考(部分)
A.1 右值引用
A.2 删除函数
A.3 默认函数
A.4 常量表达式函数
A.5 Lambda函数
A.6 变参模板


1.19.7
1.19.8
1.19.9
1.19.10
1.20
1.21
1.22
1.22.1
1.22.2
1.22.3
1.22.4
1.22.5
1.22.6
1.22.7
A.7 自动推导变量类型
A.8 线程本地变量
A.9 模板类参数的推导
A.10 本章总结
附录B 并发库简要对比
附录C 消息传递框架与完整的ATM示例
附录D C++线程类库参考
D.1 chrono头文件
D.2 condition_variable头文件
D.3 atomic头文件
D.4 future头文件
D.5 mutex头文件
D.6 ratio头文件
D.7 thread头文件


C++ Concurrency In Action
SECOND EDITION
作者:Anthony Williams
译者:陈晓伟
Git Hash:8588d16f
翻译是译者用自己的思想,换一种语言,对原作者想法的重新阐释。鉴于我的学识所限,误解和错译在
所难免。如果你能买到本书的原版,且有能力阅读英文,请直接去读原文。因为与之相较,我的译文可
能根本不值得一读。
— 云风,程序员修炼之道第2版译者
本书概述
作为对《C++ Concurrency in Action - SECOND EDITION》的中文翻译。
第二版根据C++14和C++17标准进行更新和修订,涵盖了所有标准中最新的改动!本次修订版所要解答的问
题是,如何用C++17写出优雅且健壮的多线程应用,并告诉你所有的细节!
技术方面
当需要应用有足够快的运行速度的时候,您应该选择C++。设计良好的C++并发程序将会充分利用资源,并运
行的更快。C++17可以使用多线程或多处理器,使得图像处理、机器学习等性能敏感型任务更加快速的完
成。本书特别为工业级C++并发解锁了相应的特性、模式和最佳实践方式。
关于本书
C++ Concurrency in Action, Second Edition可以作为C++在编写优雅多线程应用方面的权威指南。并更新了
C++17的相关内容,其详细描述了并行开发的各个方面,从启动新线程到设计复杂的多线程算法和数据结
构。每一章中并发大师Anthony Williams都为你提供了示例和一些练习,也包括一些独到的见解,这对于开发
经验丰富人员来说可能是非常有兴趣的。
内容概述
对C++17特性全面覆盖
启动和管理线程
同步并发操作
设计并发代码
调试多线程应用
读者须知


本书为C和C++开发人员所编写。无需任何并发经验,在看本书的时候,大家都在同一“起跑线”上。
本书相关
github翻译地址:https://github.com/xiaoweiChen/CPP-Concurrency-In-Action-2ed-2019
本书源码下载地址:https://www.manning.com/downloads/1954
第一版github 翻译地址:https://github.com/xiaoweiChen/Cpp_Concurrency_In_Action
英文原版PDF:https://ru.b-ok2.org/book/3688262/d57395?dsource=recommend
不错的笔记:https://github.com/downdemo/Cpp-Concurrency-in-Action-2ed


第一版的赞许
对C++多线程最好的介绍。
—Scott Meyers, 《Effective C++》和《More Effective C++》的作者
简化了C++的多线程。
—Rick Wagner, 红帽开发者
真的很烧脑!
—Joshua Heyer, 英格索兰
展示了如何使用并发。
—Roger Orr, OR/2 Limited
有深度,且权威的C++并发标准指南。
—Neil Horlock, 瑞士信贷银行主任
C++开发者应该去认真的看这本书。
—Dr. Jamie Allsop, 研发总监


前言
与多线程的邂逅是在毕业后的第一份工作中。那时,正在写一个填充数据库的程序,需要处理的数据量很
大,每条记录都是独立的,并且需要在插入数据库之前,对数据量进行合理分配。为了充分利用10核
UltraSPARC CPU(Ultra Scalable Processor ARChitecture,究极可扩处理器架构(大端)),我们使用了多线
程,每个线程会处理所要记录的数据。我们使用C++和POSIX线程库完成编码,也遇到了一些问题——当
时,多线程对于我们来说是一个新事物——最后还是完成了。也是做这个项目的时候,我开始注意C++标准
委员会和刚刚发布的C++标准。
我对多线程和并发有着浓厚的兴趣。虽然,别人觉得多线程和并发难用、复杂,还会让代码出现各种各样的
问题,不过,在我看来这是一种强有力的工具,能充分使用硬件资源,让程序运行的更快。
从那以后,我开始使用并发在单核机器上对应用性能和响应时长进行改善,多线程可以帮助你隐藏一些耗时
的操作,比如I/O操作。同时,我也学着在操作系统上使用多线程,并且了解Intel CPU如何处理任务切换。
同时,对C++的兴趣让我与ACCU有了联系,之后是BSI(英国标准委员会)中的C++标准委员会,还有Boost。
也是因为兴趣的原因,我参与了Boost线程库的初期开发(虽然初期版本已被放弃)。我曾是Boost线程库的主要
开发者和维护者,现在这项任务已经交给了其他人。
作为C++标准委员会的一员,希望改善现有标准的缺陷和不足,并为新标准提出建议(新标准命名为C++0x是
希望它能在2009年发布,不过最后因为2011年才发布,所以官方命名为C++11)。我也参与很多BSI的工作,
并且也为自己的建议起草建议书。因为我起草及合著的多线程和并发相关的草案,将会成为新标准的一部
分,所以当委员会将多线程提上C++标准的日程时,我高兴得差点飞起来。我也会继续关注并参与C++17标
准并行部分的修订、并发技术标准(的扩展),以及给未来并发标准的一些建议。新标准将我(计算机相关)的两
大兴趣爱好——C++和多线程——结合起来,想想还有点小激动。
本书旨在教会其他C++开发者如何安全、高效地使用C++17线程库和并发技术标准(的扩展)。希望我对C++和
多线程的热情,也能感染读者们。


感谢Kim, Hugh和 Erin
首先,要对我的妻子Kim说声“感谢!”,感谢她在我写书时给予我的爱与支持。撰写第一版时,写书占用了我
很大一部分空闲时间,当然第二版也需要大量的时间投入。如果没有她的耐心、支持和理解,我肯定没有办
法完成本书。
其次,我要感谢曼宁团队的Marjan Bace(出版商)、Michael Stephens(副出版商)、Cynthia Kane(我的研发编
辑)、Aleksandar Dragosavljevic ́(审核编辑)、Safis Editing和Heidi Ward(文案编辑),还有Melody Dolab(校对
员),他们使得本书出版成为可能。如果没有他们,你就不可能读到这本书。
当然,还要感谢C++委员会的其他成员,他们在多线程方面贡献了很多篇论文:Andrei Alexandrescu, Pete
Becker, Bob Blainer, Hans Boehm, Beman Dawes, Lawrence Crowl, Peter Dimov, Jeff Garland, Kevlin
Henney, Howard Hinnant, Ben Hutchings, Jan Kristofferson, Doug Lea, Paul McKenney, Nick McLaren,
Clark Nelson, Bill Pugh, Raul Silvera, Herb Sutter, Detlef Vollmann和Michael Wong,以及那些对论文进行评
论的人们,那些在委员会上讨论并发问题的人们,帮助在C++11、C++14、C++17标准和并发技术标准中支
持并发和多线程的人们。
最后,感谢给本书建议的人们:Dr. Jamie Allsop, Peter Dimov, Howard Hinnant, Rick Molloy, Jonathan
Wakely, and Dr. Russel Winder(特别感谢Russel博士的仔细校对),Frédéric Flayol作为技术教研员,在本书
的制作过程中,仔细的检查了所有内容,以确保终稿中没有明显的错误。
此外,我还要感谢第二版的评审小组:Al Norman, Andrei de Araújo Formiga, Chad Brewbaker, Dwight
Wilkins, Hugo Filipe Lopes, Vieira Durana, Jura Shikin, Kent R.Spillner, Maria Gemini, Mateusz Malenta,
Maurizio Tomasi, Nat Luengnaruemitchai, Robert C. Green II, Robert Trausmuth, Sanchir Kartiev和 Steven
Parr。还要感谢花时间阅读MEAP版的读者们,感谢你们指出书中的错误和需要特别说明的部分。


关于本书
本书是使用并发和多线程的指导书籍,并基于C++最新标准(11,14,17)。从最基本
的 std::thread , std::mutex 和 std::async 的使用,到复杂的原子操作和内存模型。
路线图
前4章,介绍了标准库中的各种工具,展示使用方法。
第5章,涵盖了内存模型和原子操作,包括原子操作如何对执行顺序进行限制(这章标志着介绍部分的结束)。
第6、7章,开始讨论高级主题,如何使用基本工具去构建复杂的数据结构——第6章是基于锁的数据结构,第
7章是无锁数据结构。
第8章,针对设计多线程代码给了一些指导意见,覆盖了性能问题和并行算法。
第9章,线程管理——线程池,工作队列和中断操作。
第10章,介绍C++17中标准库算法对并行性的支持。
第11章,测试和调试——Bug类型,定位Bug的技巧,以及如何进行测试等等。
附录,包括新标准中语言特性的简要描述,主要是与多线程相关的特性,以及在第4章中提到的消息传递库的
实现细节和C++17线程库的完整的参考。
适读人群
如果你正在用C++写一个多线程程序,可以阅读本书。如果你正在使用C++标准库中新的多线程工具,可以从
本书中得到一些指导。如果你正在使用其他线程库,本书的建议和技术指导也很值得参考。
阅读本书需要有较好的C++基础,关于多线程编程的知识或者经验不是必须的。
如何阅读本书
如果从来没有写过多线程代码,我建议从头到尾阅读本书,可以跳过第5章中的较为细节的部分。
第7章内容依赖于第5章中的内容,如果跳过了第5章,应该在读第7章前,读一下第5章。
如果没有用过C++11,可以先看一下附录,当在正文中遇到一些没见过的工具或特性时,可以随时回看附
录。
即使有不同环境下写多线程代码的经验,开始的章节仍有必要浏览一下,这样就能清楚地知道,你所熟知的
工具在C++的新标准中对应于哪些工具。如果使用原子变量去做一些底层工作,第5章必读。第8章有关
C++多线程的异常和安全性的内容很值得一看。如果对某些关键词比较感兴趣,索引和目录能够帮你快速找
到相应的内容。
你可能喜欢回顾主要的章节,并用自己的方式阅读示例代码。虽然你已经了解C++线程库,但附录D还是很有
用。例如,查阅每个类和函数的细节。
代码公约和下载


为了区分普通文本,清单和正文中的中的所有代码都采用 像这样的固定宽度的字体 。许多清单都伴随着代码注
释,突出显示重要的概念。在某些情况下,你可以通过页下给出的快捷链接进行查阅。
本书所有实例的源代码,可在出版商的网站上进行下载:www.manning.com/books/c-plus-plus-concurrency
in-action-second-edition。
你也可以从github上下载源码:https://github.com/anthonywilliams/ccia_code_samples。
软件需求
使用书中的代码,需要一个较新的C++编译器(要支持C++17语言的特性(见附录A)),还需要C++支持标准线程
库。
写本书的时候,最新版本的g++、clang++和Microsoft Visual Studio都对C++17的标准现成库进行了实现。他
们也会支持附录中的大多数语言特性,那些目前还不被支持的特性也会逐渐被支持。
我的公司Software Solutions Ltd,销售C++11标准线程库的完整实现,其可以使用在一些旧编译器上,以及
为新版本的clang、gcc和Microsoft Visual Studio实现的并发技术标准[1]。这个线程库也可以用来测试本书中
的例子。
Boost线程库[2]提供的API,以及可移植到多个平台。本书中的大多数例子将 std:: 替换为 boost:: ,
再 #include 引用相应的头文件,就能使用Boost线程库来运行。新标准中,部分编译器可能还不支持(例
如 std::async ),或在Boost线程库中有着不同名字(例如: boost::unique_future )。
作者在线
购买C++ Concurrency in Action就能访问曼宁(Manning Publications)的私人网络论坛,在那里可以对本书做
一些评论,问一些技术问题,获得作者或其他读者的帮助。
为了能够访问论坛和订阅它的内容,在浏览器地址中输入 https://www.manning.com/books/c-plus-plus
concurrency-in-action-second-edition 。
https://forums.manning.com/forums/about 页面将告诉你如何注册之后访问论坛,你能获得什么样的帮助,还
有论坛中的一些规则。
曼宁保证为本书的读者提供互相交流,以及和作者交流的平台。虽然曼宁自愿维护本书论坛,但不保证这样
的平台不会收取任何的费用。所以,可以给作者提一些有挑战性的问题,免得这样的地方白白浪费。
[1] The just::thread implementation of the C++ Standard Thread Library, http://www.stdthread.co.uk.
[2] The Boost C++ library collection, http://www.boost.org.


Anthony Williams是一名居住在英国的开发者、顾问和培训师,有超过20年的C++开发经验。从2001年开
始,他就是BSI C++论坛的积极水友,也是许多C++标准委员会提案的作者或合著者,其提交的提案让C++11
标准库中包含有线程库。他还致力于为C++并行工具库添加更多强有力的工具,包括给标准提出建议,并实
现这些工具,为的是将just::thread这种专业级扩展,从商业线程库添加到C++标准线程库中。


封面图片介绍
本书的封面图片的标题是“日本女性的着装”(Habit of a Lady of Japan)。这张图源自Thomas Jefferys所著的
《不同民族服饰的收藏》(Collection of the Dress of Different Nations)[1]第四卷(大概在1757年到1772年间出
版)。Thomas收集的服饰包罗万象,他的绘画优美而又细腻,对欧洲戏剧服装设计产生了长达200多年的影
响。服饰中包含着一个文明的过去和现在,不同时代中各个国家的习俗通过不同的服饰栩栩如生地呈现在伦
敦剧院的观众面前。
从上个世纪以来,着装风格已经发生了很多变化,各个国家和区域之间巨大的差异逐渐消失。现在已经很难
分辨出不同洲不同地区的人们的着装差异。或许,我们放弃了这种文化上的差异,得到的却是更加丰富多彩
的个人生活——或者说是一种更加多样有趣、更快节奏的科技生活。
在各种计算机图书铺天盖地、让人难以分辨的时代,Manning出版社正是为了赞美计算机行业中的创新性和开
拓性,才选用了这个重现两个世纪之前丰富多样的地域风情的图片。
[1] 《iPhone与iPad开发实战》使用了书中的另一张图片,感兴趣的同学可以去图灵社区进行试读(只免费提供
第1章内容),本章翻译复制了这本书翻译的部分内容


第1章 你好,并发世界!
主要内容
定义并发和多线程
使用并发和多线程
C++的并发史
简单的C++多线程
距初始C++标准(1998年)发布后的13年后,标准委员对C++进行了重大的修改。新标准(也称C++11或C++0x)
在2011年发布,一系列的修改让C++编程更加简单和高效。同时,委员会也确立了标准更新模式——每三年
发布一个新标准。从模式确立至今,委员会已经发布了两个标准:2014年的C++14标准和2017的C++17标
准,以及若干个C++技术规范标准扩展。
其中最重要的特性就是对多线程的支持。C++标准第一次包含了多线程,并在标准库中提供了多线程组件,
这让使用C++编写与平台无关的多线程程序成为可能,也为可移植性提供了强有力的保证。与此同时,开发
者们为提高应用的性能,对并发的关注也是与日俱增,特别在多线程方面。在C++11的基础上,C++14、
C++17标准,以及一些技术规范标准,都在为C++的多线程和并发添砖加瓦。
本书会使用C++11多线程来编写并发程序,并介绍相关的语言特性和工具。本章以“为什么要使用并发”作为起
始点,会对“什么情况下不使用并发”进行阐述,并且对C++的并发方式进行总结。最后,以一个简单的并发实
例结束这一章。后面的章节中,会有更多的例子,以便大家对线程库进行更加深入的了解。


1.1 何谓并发
并发,指两个或两个以上的独立活动同时发生。
并发在生活中随处可见,我们可以一边走路一边说话,也可以两只手同时做不同的动作,还有每个人都过着
相互独立的生活——当我在游泳的时候,你可以看球赛等等。
1.1.1 计算机的并发
计算机的并发,指在单个系统里同时执行多个独立的任务。
并发在计算机领域不是一个新鲜事物。很多年前,一台计算机就能通过多任务操作系统的切换功能,同时运
行多个应用,并且多处理器服务器很早就实现了并行计算。那并发为什么在计算机领域越来越流行呢?—
真正的并行。
以前,大多数计算机只有一个处理器,具有单个处理单元或核芯。这种机器只能在某一时刻执行一个任务,
不过可以在单位时间内对任务进行多次切换。通过“这个任务做一会,另一个任务再做一会儿”的方式,让任务
看起来是并行的,这种方式称为任务切换。如今,这样的方式仍称为“并发“,因为任务切换得太快,以至于无
法感觉到任务会暂时挂起。任务切换会给用户造成一种“并发的假象”,任务切换和真正并发执行相比,行为上
还是有着微妙的不同。
多核计算机用于高性能计算已有多年。基于单芯多核处理器(多核处理器)的台式机,也越来越大众化。无论有
多少个处理器,这些机器都能够真正的并行多个任务,我们称其为”硬件并发“。
图1.1显示了处理两个任务时的理想情景,每个任务被分为10个相等大小的子任务块。在一个双核机器上,每
个任务可以在各自的处理核心上执行。在单核机器上做任务切换时,每个任务的块交替进行,但中间有一小
段分隔(图中所示灰色分隔条),这表示切换任务的开销。进行任务切换时,操作系统必须保存当前任务CPU的
状态和指令指针,并计算要切换到哪个任务,再将切换到的任务加载处理器中。CPU可能要将新任务的指令
和数据载入到缓存,这会让CPU停止执行指令,从而造成的更多的延迟。
图 1.1 并发的两种方式:真正并行 vs. 任务切换
有些处理器可以在一个核心上执行多个线程,但硬件并发在多处理器上效果更加显著。硬件线程最重要的是
数量,也就是可以并发运行独立任务的数量。即便是硬件并发的系统,也有比硬件“可并行最大任务数”还要多
的任务需要执行,所以任务切换在这些情况下仍然适用。例如,一个台计算机上可能会有成百上千个的任务
在运行,即便是在计算机处于空闲时,还是会有后台任务在运行。正是任务切换使得这些后台任务可以运
行,这样系统使用者就可以同时运行文字处理器、编译器、编辑器和Web浏览器了。
图1.2显示了四个任务在双核处理器上的任务切换,仍是将任务整齐地划分为同等大小子任务块的理想情况。
实际上,许多因素会使得任务分割不均或调度不规则。


图 1.2 四个任务在两个核心之间的切换
无论应用是在单核处理器,还是多核处理器上运行,不论是任务切换,还是硬件并发,这里提到的技术、功
能和类(本书所涉及的)都会涉及。如何使用并发,很大程度上取决于可用的硬件并发。
1.1.2 并发的方式
试想当两个程序员在两个独立的办公室一起做一个软件项目,他们可以安静地工作、互不干扰,并且人手一
套参考手册。但沟通起来就有些困难,比起可以直接交谈,他们必须使用电话、电子邮件或到对方的办公室
进行面对面交流。并且,管理两个办公室需要有一定的经费支出,还需要购买多份参考手册。
假设,让开发人员同在一间办公室办公,他们可以自由的对某个程序设计进行讨论,也可以在纸或白板上绘
制图表,对设计观点进行辅助性阐释。现在,只需要管理一个办公室和一套参考资料就够了。遗憾的是,开
发人员可能难以集中注意力,并且还可能存在资源共享的问题(比如,“参考手册哪去了?”)
以上两种方法,描绘了并发的两种基本途径。开发人员代表线程,办公室代表进程。第一种方式是每个进程
只要一个线程,这就类似让每个开发人员拥有自己的办公室。而第二种方式是每个进程有多个线程,如同一
个办公室里有两个开发人员。让我们在一个应用中,简单的分析一下这两种方式。
多进程并发
使用并发的第一种方式,是将应用程序分为多个独立的进程同时运行,就像同时进行网页浏览和文字处理一
样。如图1.3所示,独立的进程可以通过进程间的通信渠道传递讯息(信号、套接字、文件、管道等等)。不
过,这种进程间的通信通常非常复杂,或是速度很慢。这是因为操作系统会对进程进行保护,以避免一个进
程去修改另一个进程的数据。还有一个缺点是运行多个进程的固定开销:需要时间启动进程,操作系统需要
资源来管理进程等等。
图 1.3 一对并发运行的进程之间的通信
当然,以上的机制也不是一无是处:操作系统在进程间提供了保护和更高级别的通信机制,可以更容易编写
安全的并发代码。实际上,在类似于Erlang的编程环境中,会将进程作为并发的基础块。


使用多进程实现并发还有一个优势——可以使用远程连接(可能需要联网)的方式,在不同的机器上运行独立的
进程。虽然,这增加了通信成本,但在设计精良的系统中,这种低成本方案可提高程序的并行可用性和性
能。
多线程并发
并发的另一个方式,在单进程中运行多个线程。线程很像轻量级的进程:每个线程相互独立运行,并且可以
在不同的指令序列中运行。不过,进程中的所有线程都共享地址空间,并且能访问到大部分数据———全局
变量仍然是全局的,指针、对象的引用或数据可以在线程之间传递。虽然,进程之间通常共享内存,但同一
数据的内存地址在不同的进程中不相同,所以这种共享难以建立和管理。图1.4展示了一个进程中的两个线
程,正在通过共享内存进行通信。
图 1.4 同一进程中,一对并发线程间的通信
地址空间共享,以及缺少线程间的数据保护,使得操作系统记录的工作量减小,所以使用多线程的开销远远
小于多进程。不过,共享内存的灵活性是有代价的:如果多个线程访问数据,那么必须确保每个线程所访问
到的数据一致,这就需要对线程通信做大量的工作。
多个单线程/进程间的通信,要比单一进程中多线程通信的开销大,若不考虑共享内存可能带来的问题,多线
程将会成为主流语言(包括C++)更青睐的并发方式。此外,C++标准并未对进程通信提供原生支持,所以实现
会依赖于平台相关的API。因此,本书只关注多线程的并发,之后所提到“并发”,均为多线程实现。
多线程应用中,还有一种方式:并行。
1.1.3 并发与并行
对多线程来说,这两个概念大部分是重叠的。对于很多人来说,它们没有什么区别。这两个词是用来描述硬
件同时执行多个任务的方式,而“并行”更加注重性能。使用硬件提高数据处理速度时,会讨论程序的并行性。
当关注重点在于任务分离或任务响应时,会讨论程序的并发性。这两个术语存在的目的,就是为了区别多线
程中不同的关注点。
了解了并发后,来看看为什么要使用并发。


1.2 为什么使用并发
原因有二:分离关注点(SOC)和性能(或者可能是两个都有。当然,除了“我乐意”这样的原因)。
1.2.1 分离关注点
编写软件时,分离关注点是个好办法。通过将相关的代码与无关的代码分离,可以使程序更容易理解和测
试,从而减少出错的可能。即使一些操作需要同时进行,依旧可以使用并发,分离不同的功能区域。若不显
式地使用并发,就得编写一个任务切换机制,或者在操作中主动地调用一段不相关的代码。
假设有一个用户界面的处理密集型应用——DVD播放程序。这样的应用程序,应具备这两种功能:一,要从
光盘中读出数据,对图像和声音进行解码,之后把解码出的信号输出至视频和音频硬件中进行处理,从而实
现DVD的播放;二,接收来自用户的输入,当用户单击“暂停”、“返回菜单”或“退出”按键的时候执行对应的操
作。当应用是单个线程时,应用需要在回放期间定期检查用户的输入,这就需要把“DVD播放”代码和“用户界
面”代码放在一起。如果使用多线程方式来分离这些关注点,“用户界面”代码和“播放DVD”代码不需要放在一
起:一个线程可以处理“用户界面”,另一个进行“播放DVD”。它们之间会有交互(用户点击“暂停”),不过任务需
要人为的进行关联。
这会带来响应上的错觉,因为用户界面线程通常可以立即响应用户的请求,尽管当请求传递给工作中的线程
时,其响应可能只是显示“忙碌中”的光标或“请等待”的消息。同理,独立的线程通常用来执行那些必须在后台
持续运行的任务,例如:桌面搜索程序中监视文件系统变化的任务。因为交互清晰可辨,所以会使每个线程
变的更加简单。
这种情况下,对线程的划分是基于概念上的设计,所以线程数不再依赖CPU核芯数。
1.2.2 性能
重核系统已经存在了几十年,直至现今,它们也只在超级计算机、大型机和大型服务器系统中才能看到。然
而,芯片制造商越来越倾向于芯片的多核设计,即在单芯片上集成2、4、16或更多的处理器,从而获取更好
的性能。因此,多核计算机、多核嵌入式设备,现在越来越普遍。它们计算能力的提高不是使单一任务运行
的更快,而是并行多个任务。曾今,开发者无需做任何事,可以看着程序随着处理器的更新换代而变得更
快。但是现在,如Herb Sutter所说的,“没有免费的午餐了。”[1] 如果想要利用日益增长的计算能力,那就必
须设计多任务并发式软件,那些迄今都忽略并发的开发者们要上心了。
有两种利用并发来提高性能的方式:第一,将一个单个任务分成几部分并行运行,从而降低总运行时间,这
就是任务并行(task parallelism)。虽然,这听起来很直观,但是一个相当复杂的过程,因为各个部分之间可能
存在着依赖。区别可能是在过程方面——一个线程执行算法的一部分,而另一个线程执行算法的另一个部分
——或是在处理数据——每个线程在不同的数据块上执行相同的操作(第二种方式)。后一种方法被称为数据并
行(data parallelism)。
容易并行的算法称为是“易并行的”(embarrassingly parallel)。易并行算法具有良好的可扩展特性——当可用硬
件线程的数量增加时,算法的并行性也会随之增加,这种算法很好的体现了“人多力量大”。如果算法中有不易
并行的部分,可以把算法划分成固定(不可扩展)数量的并行任务。
并发提升性能的第二种方式,是利用并行来解决更大的问题:每次只处理一个文件,不如处理2个、10个或20
个。虽然,这是数据并行的一种应用(通过对多组数据同时执行相同的操作),但着重点不同。处理等量数据仍
然需要同样的时间,但现在在相同的时间内处理了更多的数据。这种方法也有限制,并非所有情况下都是有


效的。不过,这种方法所带来的吞吐量提升,可以让某些功能成为可能——如果图片的不同区域能被并行地
处理,程序就可以处理更高分辨率的视频。
1.2.3 什么时候不使用并发
知道何时不使用并发与知道何时使用一样重要。
不使用并发的唯一原因就是收益比不上成本。使用并发的代码在很多情况下难以理解,因此编写和维护多线
程代码会产生脑力成本,而增加的复杂性也可能会引起更多的错误。除非潜在的性能增益足够大或关注点分
离地足够清晰,能抵消为确保正确开发所需的额外时间,以及维护代码的额外成本;否则,勿用并发。
同样地,性能增益可能会小于预期。启动线程时存在固有开销,因为操作系统需要分配内核资源和堆栈空
间,才能把新线程加入调度器中。如果在线程上的任务完成得很快,那么实际执行任务的时间要比启动线程
的时间小很多,这会导致应用的整体性能不如直接使用单线程。
此外,线程的资源有限。如果太多的线程同时运行,则会消耗很多操作系统资源,从而使得操作系统整体上
运行得更加缓慢。不仅如此,因为每个线程都需要一个独立的堆栈,所以运行太多的线程也会耗尽进程的可
用内存或地址空间。对于一个可用地址空间为4GB(32bit)的架构来说,这的确是个问题:如果每个线程都有一
个1MB的堆栈(很多系统都会这样分配),那么4096个线程将会用尽所有地址空间(不会给代码、静态数据或者
堆数据留有任何空间)。即便是64位(或者更大)的系统,不存在这种地址空间限制,但其他资源也是有限的:
如果你运行了太多的线程,也会出问题。尽管线程池(参见第9章)可以用来限制线程的数量,但也并不是什么
灵丹妙药。
当客户端/服务器(C/S)应用在服务端为每一个链接启动一个独立的线程时,对于少量链接没有问题,但当用于
需要处理大量链接的高需求服务器时,就会因为线程太多而耗尽系统资源。这种场景下,使用线程池可以对
性能进行优化(参见第9章)。
最后,运行越多的线程,操作系统就需要越多的上下文切换,每一次切换都需要耗费时间。所以在某些时
候,增加线程实际上会降低应用的整体性能。如果试图得到系统的最佳性能,可以考虑使用硬件并发(或不
用),并调整运行线程的数量。
和所有其他优化策略一样,我们为了性能而使用并发:它可以大幅度提高应用的性能,但也可能让代码更加
复杂,难以理解,并且更容易出错。因此,应用中只有性能关键部分,才值得并发化。当然,如果性能收益
仅次于设计清晰或分离关注点,也可以使用多线程。
既然已经看到了这里,那无论是为了性能、关注点分离,亦或是因为多线程星期一(multithreading Monday)
(译者:可能是学习多线程的意思),你应该确定要在应用中使用并发了。
好!那对于C++开发者来说,多线程意味着什么呢?
[1] “The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software,” Herb Sutter, Dr. Dobb’s
Journal, 30(3), March 2005. http://www.gotw.ca/publications/concurrency-ddj.htm.


1.3 并发和多线程
使用C++11标准,可以编写不依赖平台扩展的多线程代码。了解C++线程库前,先来了解一下C++多线程的发
展史。
1.3.1 C++多线程历史
C++98(1998)标准不承认线程的存在,并且各种语义以顺序抽象的形式编写。不仅如此,也没有内存模型,
所以C++98标准在缺少编译器扩展的情况下,没办法编写多线程应用。
当然,编译器供应商可以自由地向语言添加扩展,C语言中流行的多线程API——POSIX标准中的C标准和
Microsoft Windows API——很多C++编译器供应商,通过各种平台相关的扩展来支持多线程。这种支持受限
于平台,并且需要相应平台的运行库(例如,异常处理机制的代码)能在多线程情况下正常工作。因为编译器和
处理器的实际表现很不错,所以在少数编译器供应商提供正式的多线程内存模型之前,开发者们已经写了很
多的C++多线程程序了。
由于不满足于使用平台相关的API来处理多线程,C++开发者们希望使用面向对象的多线程工具。像MFC这样
的应用框架,和Boost和ACE这样的通用库,这些库提供了很多简化任务的多线程工具。各种库在细节方面差
异很大,但在启动线程的方面,却大同小异。其使用一种便利的设计,也就是使用带锁的获取资源即初始化
(RAII, Resource Acquisition Is Initialization)的方式。
编写多线程代码需要扎实的编程基础,当前的很多C++编译器为多线程编程者提供了对应的API,还有一些与
平台无关的C++库。这样,开发者们就可以通过这些API来实现多线程。不过,由于缺乏统一的标准,以及内
存模型,就会产生一些问题,这些问题在跨平台的多线程应用上表现得尤为明显。
1.3.2 支持并发
这些随着C++11标准的发布而改变,新标准中不仅有了全新的内存模型,C++标准库也扩展了:管理线程(参
见第2章)、保护共享数据(参见第3章)、线程间同步操作(参见第4章),以及原子操作(参见第5章)。
标准线程库很大程度上,基于之前C++库的经验积累。特别是Boost线程库,作为新标准库的很多类与Boost
库中的相关类有着相同名称和结构。随着C++标准的进步,Boost线程库也随着C++标准在许多方面做出改
变,这样之前使用Boost的用户会发现自己非常熟悉C++11的线程库。
支持并发仅是C++11标准的变化之一,为了让开发者们的工作变得更加轻松,还有很多对于语言自身的改
善。这些内容不在本书的讨论范围内,但是其中的一些变化对线程库及其使用方式有着很大的影响。附录A会
对这些特性做一些介绍。
1.3.3 C++14和C++17对并发和并行的更多支持
C++14中为并发和并行添加了一个新的互斥量类型,用于保护共享数据(参见第3章)。C++17考虑的更多:添
加了一整套的并行算法(参见第10章)。两个标准将整个标准库进行了补强,这让书写多线程代码变得更加容
易。
之前还提到了一个并发技术标准,其描述C++标准对于函数和类的扩展,尤其是对线程同步方面(参见第4
章)。


C++新标准直接支持原子操作,允许开发者通过指定语义的方式编写代码,从而无需了解与平台相关的汇编
指令。这对于编写高效、可移植的代码来说,无疑是一个好消息。编译器不仅可以搞定具体平台,还可以编
写优化器来解释操作语义,从而让程序得到更好的优化。
1.3.4 C++线程库的效率
这是高性能计算开发者的担忧之一。为了效率,C++整合了一些底层工具。这样就需要了解使用高级工具和
使用低级工具的开销差,这个开销差就是抽象代价(abstraction penalty)。
C++标准委员会在设计标准库时,特别是线程库,就注意到了这点。目的就是在实现相同功能的前提下,确
保使用高级API和使用底层API带来的性能收益相当。这样,标准库在主流平台上都能有高效实现(带有非常低
的抽象代价)。
为了达到终极性能,需要给与硬件打交道的开发者提供足够多的底层工具。为了这个目的,形成了原子操作
库,可直接控制单个位、字节、内部线程间的同步,以及对所有变化的可见性。原子类型可以在很多地方使
用,使用新标准的代码会有更好的可移植性,并且容易维护。
标准库也提供了更高级别工具,使得编写多线程代码更加简单。因为有额外的代码需要执行,这些工具确实
会带来性能开销。总的来说,性能开销和手工编写的函数差不多,并且编译器会内联大部分代码。
某些情况下,高级工具会提供一些额外的功能。极少的情况下,一些未使用的功能会影响其他代码的性能。
如果很看重程序的性能,并且高级工具带来的开销过高,最好是通过较低层工具来实现功能。绝大多数情况
下,用过高的复杂性和过大的出错率,来交换小幅度的性能收益是不划算的。即便是瓶颈出现在C++标准库
的工具中,也可能由低劣的程序设计造成。例如,如果过多的线程竞争一个互斥单元,将会很明显的影响性
能。与其在互斥操作上耗费时间,不如重新设计,减少互斥单元上的竞争。
C++标准库没有提供所需的性能或行为时,就需要使用与平台相关的工具了。
1.3.5 平台相关的工具
虽然C++线程库为多线程和并发处理提供了较全面的工具,但某些平台会提供额外的工具。为了方便地使用
这些工具,又使用标准C++线程库,在C++线程库中提供一个 native_handle() 的成员函数,允许通过使用平台
相关API直接操作底层实现。就其本质而言,任何使用 native_handle() 执行的操作都是完全依赖于平台,这也
超出了本书(同时也是标准C++库本身)讨论的范围。
所以,使用平台相关的工具之前,先了解一下标准库能够做些什么吧。


1.4 开始入门
OK!准备一个能与C++11/C++14/C++17标准兼容的编译器。C++多线程程序是什么样子呢?其实,和其他
C++程序差不多。唯一的区别在于某些函数可以并发运行,所以需要确保共享数据在并发访问时是安全的。
当然,为了并发地运行,必须使用特定函数以及对象来管理各个线程。
1.4.1 欢迎来到并发世界
从一个经典的例子开始:一个打印“Hello World”的程序。一个非常简单的在单线程中运行的Hello World程序
如下所示,当我们谈到多线程时,可以作为一个基准。
#include <iostream>
int main()
{
std::cout << "Hello World\n";
}
这个程序所做的就是将“Hello World”写进标准输出流。让我们将它与下面清单所示的简单的“Hello,
Concurrent World”程序做个比较,它启动了一个独立的线程来显示这个信息。
代码 1.1 一个简单的Hello, Concurrent World程序:
#include <iostream>
#include <thread> // 1
void hello() // 2
{
std::cout << "Hello Concurrent World\n";
}
int main()
{
std::thread t(hello); // 3
t.join(); // 4
}
第一个区别是增加了 #include <thread> 1,包括标准库中对多线程支持的声明,管理线程的函数和类
在 <thread> 中声明(保护共享数据的函数和类在其他头文件中声明)。
其次,打印信息移到了一个独立的函数中2。因为每个线程都必须一个执行单元,新线程的执行从这里开
始。对于应用程序来说,初始线程是main(),但是对于其他线程,可以在 std::thread 对象的构造函数中指定
——本例中命名为 t 3的 std::thread 对象拥有新函数hello()作为其执行函数。
下一个区别:与直接写入标准输出或是从main()调用hello()不同,该程序启动了一个全新的线程来实现,将线
程数量一分为二——初始线程始于main(),而新线程始于hello()。
新的线程启动之后3,初始线程继续执行。如果它不等待新线程结束,就运行到main()函数结束——有可能发
生在新线程运行之前。这就是为什么在4这里调用 join() 的原因——详见第2章,这会让创建线程等
待 std::thread 对象创建的线程。


这看起来仅是为了将一条信息写入标准输出,确实如此——正如上文1.2.3节所描述的,一般来说并不值得为
了如此简单的任务而使用多线程,尤其是在这期间初始线程并没做什么。后面的章节中,将通过更加复杂的
实例来展示,在哪些情景下使用多线程更有意义。


1.5 本章总结
本章中,提及了并发与多线程的含义,以及在应用中为什么使用(或不使用)并发。还提及了多线程在C++中的
发展历程,从1998标准中完全缺乏支持,经历了各种平台相关的扩展,再到C++11/C++14/C++17标准和并发
技术规范对多线程的支持。芯片制造商选择了以多核芯的形式,使得更多任务可以同时执行的方式来增加处
理能力,而不是增加单个核心的执行速度。在这个趋势下,C++多线程来的正是时候,它使得开发者们可以
利用CPU带来的更加强大的硬件并发。
1.4节中例子,展示C++标准库中的类和函数有多么的简单。C++中使用多线程并不复杂,复杂的是如何设计
代码以实现预期的行为。
尝试了1.4节的示例后,可以了解更多实质性的内容。
第2章中,我们将了解用于管理线程的类和函数。


第2章 线程管理
主要内容
启动新线程
等待与分离
唯一标识符
先做点什么呢?启动线程、结束线程,还是管理线程?C++标准库中只管理与 std::thread 关联的线程。不过,
标准库很灵活,管理起来不太容易。
本章开始将会介绍,如何启动一个线程,并等待这个线程结束,或放在后台运行。再介绍如何给已经线程函
数传递参数,以及将线程的所有权进行移交。最后,再来了解线程数量和特殊线程。


2.1 线程的基本操作
每个程序至少有一个执行main()函数的线程,其他线程与主线程同时运行。如main()函数执行完会退出一样,
线程执行完函数也会退出。为线程创建 std::thread 对象后,需要等待这个线程结束。那么,就先来启动线
程。
2.1.1 启动线程
第1章中,线程在 std::thread 对象创建时启动,通常使用的是无参数无返回的函数。这种函数在执行完毕,
线程也就结束了。一些情况下,任务函数对象需要通过某种通讯机制进行参数的传递,或者执行一系列独立
操作,通过通讯机制传递信号让线程停止。先放下这些特殊情况不谈,简单来说,使用C++线程库启动线
程,就是构造 std::thread 对象:
void do_some_work();
std::thread my_thread(do_some_work);
这里需要包含 <thread> 头文件, std::thread 可以通过有函数操作符类型的实例进行构造:
class background_task
{
public:
void operator()() const
{
do_something();
do_something_else();
}
};
background_task f;
std::thread my_thread(f);
代码中,提供的函数对象会复制到新线程的存储空间中,函数对象的执行和调用都在线程的内存空间中进
行。
有件事需要注意,当把函数对象传入到线程构造函数中时,需要避免“最令人头痛的语法解析”(C++’s most
vexing parse, 中文简介)。如果你传递了一个临时变量,而不是一个命名的变量。C++编译器会将其解析为函
数声明,而不是类型对象的定义。
std::thread my_thread(background_task());
这相当与声明了一个名为my_thread的函数,这个函数带有一个参数(函数指针指向没有参数并返回
background_task对象的函数),返回一个 std::thread 对象的函数。
使用在前面命名函数对象的方式,或使用多组括号1,或使用统一的初始化语法2,都可以避免这个问题。
如下所示:


std::thread my_thread((background_task())); // 1
std::thread my_thread{background_task()}; // 2
Lambda表达式也能避免这个问题。Lambda表达式是C++11的一个新特性,允许使用一个可以捕获局部变量
的局部函数(可以避免传递参数,参见2.2节)。想要详细了解Lambda表达式,可以阅读附录A的A.5节。之前的
例子可以改写为Lambda表达式的方式:
std::thread my_thread([]{
do_something();
do_something_else();
});
线程启动后是要等待线程结束,还是让其自主运行。当 std::thread 对象销毁之前还没有做出决定,程序就会
终止( std::thread 的析构函数会调用 std::terminate() )。因此,即便是有异常存在,也需要确保线程能够正确
汇入(joined)或分离(detached)。
如果不等待线程汇入 ,就必须保证线程结束之前,访问数据的有效性。这不是一个新问题——单线程代码
中,对象销毁之后再去访问,会产生未定义行为——不过,线程的生命周期增加了这个问题发生的几率。
这种情况很可能发生在线程还没结束,函数已经退出的时候,这时线程函数还持有函数局部变量的指针或引
用。
代码2.1 函数已经返回,线程依旧访问局部变量
struct func
{
int& i;
func(int& i_) : i(i_) {}
void operator() ()
{
for (unsigned j=0 ; j<1000000 ; ++j)
{
do_something(i); // 1 潜在访问隐患:空引用
}
}
};
void oops()
{
int some_local_state=0;
func my_func(some_local_state);
std::thread my_thread(my_func);
my_thread.detach(); // 2 不等待线程结束
} // 3 新线程可能还在运行
代码中,已经决定不等待线程(使用了detach()2),所以当oops()函数执行完成时3,线程中的函数可能还在
运行。如果线程还在运行,就会去调用do_something(i)1,这时就会访问已经销毁的变量。如同一个单线程
程序——允许在函数完成后继续持有局部变量的指针或引用。当然,这种情况发生时,错误并不明显,会使


多线程更容易出错。运行顺序参考表2.1。
表2.1 分离线程在局部变量销毁后,仍对该变量进行访问
主线程 新线程
使用some_local_state构 造my_func
开启新线程my_thread
启动
调用func::operator()
将my_thread分离 执行func::operator();可能会在do_something中调用some_local_state的引
用
销毁some_local_state 持续运行
退出oops函数 持续执行func::operator();可能会在do_something中调用
some_local_state的引用 --> 导致未定义行为
这种情况的常规处理方法:将数据复制到线程中。如果使用一个可调用的对象作为线程函数,这个对象就会
复制到线程中,而后原始对象会立即销毁。如代码2.1所示,但对于对象中包含的指针和引用还需谨慎。使用
访问局部变量的函数去创建线程是一个糟糕的主意。
此外,可以通过join()函数来确保线程在主函数完成前结束。
2.1.2 等待线程完成
如需等待线程,需要使用join()。将代码2.1中的 my_thread.detach() 替换为 my_thread.join() ,就可以确保局部
变量在线程完成后才销毁。因为主线程并没有做什么事,使用独立的线程去执行函数变得意义不大。但在实
际中,原始线程要么有自己的工作要做,要么会启动多个子线程来做一些有用的工作,并等待这些线程结
束。
当你需要对等待中的线程有更灵活的控制时,比如:看一下某个线程是否结束,或者只等待一段时间(超过时
间就判定为超时)。想要做到这些,需要使用其他机制来完成,比如条件变量和future。调用join(),还可以清
理了线程相关的内存,这样 std::thread 对象将不再与已经完成的线程有任何关联。这意味着,只能对一个线
程使用一次join(),一旦使用过join(), std::thread 对象就不能再次汇入了。当对其使用joinable()时,将返回
false。
2.1.3 特殊情况下的等待
如前所述,需要对一个未销毁的 std::thread 对象使用join()或detach()。如果想要分离线程,可以在线程启动
后,直接使用detach()进行分离。如果等待线程,则需要细心挑选使用join()的位置。当在线程运行后产生的
异常,会在join()调用之前抛出,这样就会跳过join()。
避免应用被抛出的异常所终止。通常,在无异常的情况下使用join()时,需要在异常处理过程中调用join(),从
而避免生命周期的问题。
代码2.2 等待线程完成


struct func; // 定义在代码2.1中
void f()
{
int some_local_state=0;
func my_func(some_local_state);
std::thread t(my_func);
try
{
do_something_in_current_thread();
}
catch(...)
{
t.join(); // 1
throw;
}
t.join(); // 2
}
代码2.2中使用了 try/catch 块确保线程退出后函数才结束。当函数正常退出后,会执行到2处。当执行过程
中抛出异常,程序会执行到1处。如果线程在函数之前结束——就要查看是否因为线程函数使用了局部变量
的引用——而后再确定一下程序可能会退出的途径,无论正常与否,有一个简单的机制,可以解决这个问
题。
一种方式是使用“资源获取即初始化方式”(RAII,Resource Acquisition Is Initialization),提供一个类,在析构
函数中使用join()。如同下面代码。
代码2.3 使用RAII等待线程完成


class thread_guard
{
std::thread& t;
public:
explicit thread_guard(std::thread& t_):
t(t_)
{}
~thread_guard()
{
if(t.joinable()) // 1
{
t.join(); // 2
}
}
thread_guard(thread_guard const&)=delete; // 3
thread_guard& operator=(thread_guard const&)=delete;
};
struct func; // 定义在代码2.1中
void f()
{
int some_local_state=0;
func my_func(some_local_state);
std::thread t(my_func);
thread_guard g(t);
do_something_in_current_thread();
} // 4
线程执行到4处时,局部对象就要被逆序销毁了。因此,thread_guard对象g是第一个被销毁的,这时线程在
析构函数中被加入2到原始线程中。即使do_something_in_current_thread抛出一个异常,这个销毁依旧会发
生。
在thread_guard析构函数的测试中,首先判断线程是否可汇入1。如果可汇入,会调用join()2进行汇入。
拷贝构造函数和拷贝赋值操作标记为 =delete 3,是为了不让编译器自动生成。直接对对象进行拷贝或赋值是
很危险的,因为这可能会弄丢已汇入的线程。通过删除声明,任何尝试给thread_guard对象赋值的操作都会
引发一个编译错误。想要了解删除函数的更多知识,请参阅附录A的A.2节。
如果不想等待线程结束,可以分离线程,从而避免异常。不过,这就打破了线程与 std::thread 对象的联系,
即使线程仍然在后台运行着,分离操作也能确保 std::terminate() 在 std::thread 对象销毁时才调用。
2.1.4 后台运行线程
使用detach()会让线程在后台运行,这就意味着与主线程不能直接交互。如果线程分离,就不可能
有 std::thread 对象能引用它,分离线程的确在后台运行,所以分离的线程不能汇入。不过C++运行库保证,
当线程退出时,相关资源的能够正确回收。


分离线程通常称为守护线程(daemon threads)。UNIX中守护线程,是指没有任何显式的接口,并在后台运行
的线程,这种线程的特点就是长时间运行。线程的生命周期可能会从应用的起始到结束,可能会在后台监视
文件系统,还有可能对缓存进行清理,亦或对数据结构进行优化。另外,分离线程只能确定线程什么时候结
束,发后即忘(fire and forget)的任务使用到就是分离线程。
如2.1.2节所示,调用 std::thread 成员函数detach()来分离一个线程。之后,相应的 std::thread 对象就与实际
执行的线程无关了,并且这个线程也无法汇入:
std::thread t(do_background_work);
t.detach();
assert(!t.joinable());
为了从 std::thread 对象中分离线程,不能对没有执行线程的 std::thread 对象使用detach(),并且要用同样的
方式进行检查——当 std::thread 对象使用t.joinable()返回的是true,就可以使用t.detach()。
试想如何能让一个文字处理应用同时编辑多个文档。无论是用户界面,还是在内部应用内部进行,都有很多
的解决方法。虽然,这些窗口看起来是完全独立的,每个窗口都有自己独立的菜单选项,但他们却运行在同
一个应用实例中。一种内部处理方式是,让每个文档处理窗口拥有自己的线程。每个线程运行同样的的代
码,并隔离不同窗口处理的数据。如此这般,打开一个文档就要启动一个新线程。因为是对独立文档进行操
作,所以没有必要等待其他线程完成,这里就可以让文档处理窗口运行在分离线程上。
代码2.4 使用分离线程处理文档
void edit_document(std::string const& filename)
{
open_document_and_display_gui(filename);
while(!done_editing())
{
user_command cmd=get_user_input();
if(cmd.type==open_new_document)
{
std::string const new_name=get_filename_from_user();
std::thread t(edit_document,new_name); // 1
t.detach(); // 2
}
else
{
process_user_input(cmd);
}
}
}
如果用户选择打开一个新文档,需要启动一个新线程去打开新文档1,并分离线程2。与当前线程做出的操作
一样,新线程只不过是打开另一个文件而已。所以,edit_document函数可以复用, 并通过传参的形式打开新
的文件。
这个例子也展示了传参启动线程的方法:不仅可以向 std::thread 构造函数1传递函数名,还可以传递函数所
需的参数(实参)。当然,也有其他方法可以完成这项功能,比如:使用带有数据的成员函数,代替需要传参的
普通函数。


2.2 传递参数
如代码2.4所示,向可调用对象或函数传递参数很简单,只需要将这些参数作为 std::thread 构造函数的附加
参数即可。需要注意的是,这些参数会拷贝至新线程的内存空间中(同临时变量一样)。即使函数中的参数是引
用的形式,拷贝操作也会执行。来看一个例子:
void f(int i, std::string const& s);
std::thread t(f, 3, "hello");
代码创建了一个调用f(3, "hello")的线程。注意,函数f需要一个 std::string 对象作为第二个参数,但这里使用
的是字符串的字面值,也就是 char const * 类型,线程的上下文完成字面值向 std::string 的转化。需要特别
注意,指向动态变量的指针作为参数的情况,代码如下:
void f(int i,std::string const& s);
void oops(int some_param)
{
char buffer[1024]; // 1
sprintf(buffer, "%i",some_param);
std::thread t(f,3,buffer); // 2
t.detach();
}
buffer1是一个指针变量,指向局部变量,然后此局部变量通过buffer传递到新线程中2。此时,函数 oops 可
能会在buffer转换成 std::string 之前结束,从而导致未定义的行为。因为,无法保证隐式转换的操作
和 std::thread 构造函数的拷贝操作的顺序,有可能 std::thread 的构造函数拷贝的是转换前的变量(buffer指
针)。解决方案就是在传递到 std::thread 构造函数之前,就将字面值转化为 std::string :
void f(int i,std::string const& s);
void not_oops(int some_param)
{
char buffer[1024];
sprintf(buffer,"%i",some_param);
std::thread t(f,3,std::string(buffer)); // 使用std::string,避免悬空指针
t.detach();
}
相反的情形(期望传递一个非常量引用,但复制了整个对象)倒是不会出现,因为会出现编译错误。比如,尝试
使用线程更新引用传递的数据结构:


void update_data_for_widget(widget_id w,widget_data& data); // 1
void oops_again(widget_id w)
{
widget_data data;
std::thread t(update_data_for_widget,w,data); // 2
display_status();
t.join();
process_widget_data(data);
}
虽然update_data_for_widget1的第二个参数期待传入一个引用,但 std::thread 的构造函数2并不知晓,构
造函数无视函数参数类型,盲目地拷贝已提供的变量。不过,内部代码会将拷贝的参数以右值的方式进行传
递,这是为了那些只支持移动的类型,而后会尝试以右值为实参调用update_data_for_widget。但因为函数期
望的是一个非常量引用作为参数(而非右值),所以会在编译时出错。对于熟悉 std::bind 的开发者来说,问题
的解决办法很简单:可以使用 std::ref 将参数转换成引用的形式。因此可将线程的调用改为以下形式:
std::thread t(update_data_for_widget,w,std::ref(data));
这样update_data_for_widget就会收到data的引用,而非data的拷贝副本,这样代码就能顺利的通过编译了。
如果熟悉 std::bind ,就应该不会对以上述传参的语法感到陌生,因为 std::thread 构造函数和 std::bind 的操
作在标准库中以相同的机制进行定义。比如,你也可以传递一个成员函数指针作为线程函数,并提供一个合
适的对象指针作为第一个参数:
class X
{
public:
void do_lengthy_work();
};
X my_x;
std::thread t(&X::do_lengthy_work, &my_x); // 1
这段代码中,新线程将会调用my_x.do_lengthy_work(),其中my_x的地址1作为对象指针提供给函数。也可
以为成员函数提供参数: std::thread 构造函数的第三个参数就是成员函数的第一个参数,以此类推(代码如
下,译者自加)。
class X
{
public:
void do_lengthy_work(int);
};
X my_x;
int num(0);
std::thread t(&X::do_lengthy_work, &my_x, num);


另一种有趣的情形是,提供的参数仅支持移动(move),不能拷贝。“移动”是指原始对象中的数据所有权转移给
另一对象,从而这些数据就不再在原始对象中保存(译者:比较像在文本编辑的剪切操作)。 std::unique_ptr 就
是这样一种类型(译者:C++11中的智能指针),这种类型为动态分配的对象提供内存自动管理机制(译者:类
似垃圾回收机制)。同一时间内,只允许一个 std::unique_ptr 实例指向一个对象,并且当这个实例销毁时,指
向的对象也将被删除。移动构造函数(move constructor)和移动赋值操作符(move assignment operator)允许一
个对象的所有权在多个 std::unique_ptr 实例中传递(有关“移动”的更多内容,请参考附录A的A.1.1节)。使用“移
动”转移对象所有权后,就会留下一个空指针。使用移动操作可以将对象转换成函数可接受的实参类型,或满
足函数返回值类型要求。当原对象是临时变量时,则自动进行移动操作,但当原对象是一个命名变量,转移
的时候就需要使用 std::move() 进行显示移动。下面的代码展示了 std::move 的用法,展示了 std::move 是如何
转移动态对象的所有权到线程中去的:
void process_big_object(std::unique_ptr<big_object>);
std::unique_ptr<big_object> p(new big_object);
p->prepare_data(42);
std::thread t(process_big_object,std::move(p));
通过在 std::thread 构造函数中执行 std::move(p) ,big_object 对象的所有权首先被转移到新创建线程的的内
部存储中,之后再传递给process_big_object函数。
C++标准线程库中和 std::unique_ptr 在所属权上相似的类有好几种, std::thread 为其中之一。虽
然, std::thread 不像 std::unique_ptr 能占有动态对象的所有权,但是它能占有其他资源:每个实例都负责管
理一个线程。线程的所有权可以在多个 std::thread 实例中转移,这依赖于 std::thread 实例的可移动且不可复
制性。不可复制性表示在某一时间点,一个 std::thread 实例只能关联一个执行线程。可移动性使得开发者可
以自己决定,哪个实例拥有线程实际执行的所有权。


2.3 转移所有权
假设通过新线程返回的所有权去调用一个需要后台启动线程的函数,并需要在函数中转移线程的所有权。这
些操作都要等待线程结束才能进行,并且需要线程的所有权能够进行转移。
这就是将移动操作引入 std::thread 的原因,C++标准库中有很多资源占有(resource-owning)类型,比
如 std::ifstream , std::unique_ptr 还有 std::thread 都是可移动,但不可复制。这说明执行线程的所有权可以
在 std::thread 实例中移动,下面将展示一个例子。例子中,创建了两个执行线程,并在 std::thread 实例之间
(t1,t2和t3)转移所有权:
void some_function();
void some_other_function();
std::thread t1(some_function); // 1
std::thread t2=std::move(t1); // 2
t1=std::thread(some_other_function); // 3
std::thread t3; // 4
t3=std::move(t2); // 5
t1=std::move(t3); // 6 赋值操作将使程序崩溃
首先,新线程与t1相关联1。当显式使用 std::move() 创建t2后2,t1的所有权就转移给了t2。之后,t1和执行
线程已经没有关联了,执行some_function的函数线程与t2关联。
然后,临时 std::thread 对象相关的线程启动了3。为什么不显式调用 std::move() 转移所有权呢?因为,所有
者是一个临时对象——移动操作将会隐式的调用。
t3使用默认构造方式创建4,没有与任何线程进行关联。调用 std::move() 将t2关联线程的所有权转移到t3中
5。因为t2是一个命名对象,需要显式的调用 std::move() 。移动操作5完成后,t1与执行
some_other_function的线程相关联,t2与任何线程都无关联,t3与执行some_function的线程相关联。
最后一个移动操作,将some_function线程的所有权转移6给t1。不过,t1已经有了一个关联的线程(执行
some_other_function的线程),所以这里系统直接调用 std::terminate() 终止程序继续运行。这样做(不抛出异
常, std::terminate() 是noexcept函数)是为了保证与 std::thread 的析构函数的行为一致。2.1.1节中,需要在
线程对象析构前,显式的等待线程完成,或者分离它,进行赋值时也需要满足这些条件(说明:不能通过赋新
值给 std::thread 对象的方式来"丢弃"一个线程)。
std::thread 支持移动,线程的所有权可以在函数外进行转移,就如下面程序一样。
代码2.5 函数返回 std::thread 对象


std::thread f()
{
void some_function();
return std::thread(some_function);
}
std::thread g()
{
void some_other_function(int);
std::thread t(some_other_function,42);
return t;
}
当所有权可以在函数内部传递,就允许 std::thread 实例作为参数进行传递,代码如下:
void f(std::thread t);
void g()
{
void some_function();
f(std::thread(some_function));
std::thread t(some_function);
f(std::move(t));
}
std::thread 支持移动可以创建thread_guard类的实例(定义见清单2.3),并且拥有线程所有权。当引用
thread_guard对象所持有的线程时,移动操作就可以避免很多不必要的麻烦。当某个对象转移了线程的所有
权,就不能对线程进行汇入或分离。为了确保线程在程序退出前完成,定义了scoped_thread类。现在,我们
来看一下这个类型:
代码2.6 scoped_thread的用法


class scoped_thread
{
std::thread t;
public:
explicit scoped_thread(std::thread t_): // 1
t(std::move(t_))
{
if(!t.joinable()) // 2
throw std::logic_error(“No thread”);
}
~scoped_thread()
{
t.join(); // 3
}
scoped_thread(scoped_thread const&)=delete;
scoped_thread& operator=(scoped_thread const&)=delete;
};
struct func; // 定义在代码2.1中
void f()
{
int some_local_state;
scoped_thread t(std::thread(func(some_local_state))); // 4
do_something_in_current_thread();
} // 5
与代码2.3相似,不过新线程会直接传递到scoped_thread中4,而非创建一个独立变量。当主线程到达f()末
尾时5,scoped_thread对象就会销毁,然后在构造函数中完成汇入3。代码2.3中的thread_guard类,需要
在析构中检查线程是否“可汇入”。这里把检查放在了构造函数中2,并且当线程不可汇入时抛出异常。
C++17标准给出一个建议,就是添加一个joining_thread的类型,这个类型与 std::thread 类似,不同是的添加
了析构函数,就类似于scoped_thread。委员会成员们对此并没有达成统一共识,所以这个类没有添加入
C++17标准中(C++20仍旧对这种方式进行探讨,不过名称为 std::jthread ),这个类实现起来也不是很困难。
代码2.7 joining_thread类的实现


class joining_thread
{
std::thread t;
public:
joining_thread() noexcept=default;
template<typename Callable,typename ... Args>
explicit joining_thread(Callable&& func,Args&& ... args):
t(std::forward<Callable>(func),std::forward<Args>(args)...)
{}
explicit joining_thread(std::thread t_) noexcept:
t(std::move(t_))
{}
joining_thread(joining_thread&& other) noexcept:
t(std::move(other.t))
{}
joining_thread& operator=(joining_thread&& other) noexcept
{
if(joinable()){
join();
}
t = std::move(other.t);
return *this;
}
joining_thread& operator=(std::thread other) noexcept
{
if(joinable())
join();
t=std::move(other);
return *this;
}
~joining_thread() noexcept
{
if(joinable())
join();
}
void swap(joining_thread& other) noexcept
{
t.swap(other.t);
}
std::thread::id get_id() const noexcept{
return t.get_id();
}
bool joinable() const noexcept
{
return t.joinable();
}
void join()
{


t.join();
}
void detach()
{
t.detach();
}
std::thread& as_thread() noexcept
{
return t;
}
const std::thread& as_thread() const noexcept
{
return t;
}
};
std::thread 对象的容器,如果这个容器是移动敏感的(比如,标准中的 std::vector<> ),那么移动操作同样适
用于这些容器。了解这些后,就可以写出类似代码2.7中的代码,代码量产了一些线程,并且等待它们结束。
代码2.8 量产线程,等待它们结束
void do_work(unsigned id);
void f()
{
std::vector<std::thread> threads;
for (unsigned i = 0; i < 20; ++i)
{
threads.emplace_back(do_work,i); // 产生线程
}
for (auto& entry : threads) // 对每个线程调用 join()
entry.join();
}
我们有时需要线程去分割一个算法的工作总量,所以在算法结束的之前,所有的线程必须结束。代码2.8中线
程所做的工作都是独立的,并且结果仅会受到共享数据的影响。如果f()有返回值,这个返回值就依赖于线程
得到的结果。写入返回值之前,程序会检查使用共享数据的线程是否终止。结果在不同线程中转移的方案,
会在第4章中再次讨论。
将 std::thread 放入 std::vector 是向线程自动化管理迈出的第一步:并非为这些线程创建独立的变量,而是把
它们当做一个组。创建一组线程(数量在运行时确定),而非代码2.8那样创建固定数量的线程。


2.4 确定线程数量
std::thread::hardware_concurrency() 在新版C++中非常有用,其会返回并发线程的数量。例如,多核系统中,
返回值可以是CPU核芯的数量。返回值也仅仅是一个标识,当无法获取时,函数返回0。
代码2.9实现了并行版的 std::accumulate 。代码将整体工作拆分成小任务,交给每个线程去做,并设置最小任
务数,避免产生太多的线程,程序会在操作数量为0时抛出异常。比如, std::thread 无法启动线程,就会抛
出异常。
代码2.9 并行版的 std::accumulate


template<typename Iterator,typename T>
struct accumulate_block
{
void operator()(Iterator first,Iterator last,T& result)
{
result=std::accumulate(first,last,result);
}
};
template<typename Iterator,typename T>
T parallel_accumulate(Iterator first,Iterator last,T init)
{
unsigned long const length=std::distance(first,last);
if(!length) // 1
return init;
unsigned long const min_per_thread=25;
unsigned long const max_threads=
(length+min_per_thread-1)/min_per_thread; // 2
unsigned long const hardware_threads=
std::thread::hardware_concurrency();
unsigned long const num_threads= // 3
std::min(hardware_threads != 0 ? hardware_threads : 2, max_threads);
unsigned long const block_size=length/num_threads; // 4
std::vector<T> results(num_threads);
std::vector<std::thread> threads(num_threads-1); // 5
Iterator block_start=first;
for(unsigned long i=0; i < (num_threads-1); ++i)
{
Iterator block_end=block_start;
std::advance(block_end,block_size); // 6
threads[i]=std::thread( // 7
accumulate_block<Iterator,T>(),
block_start,block_end,std::ref(results[i]));
block_start=block_end; // 8
}
accumulate_block<Iterator,T>()(
block_start,last,results[num_threads-1]); // 9
for (auto& entry : threads)
entry.join(); // 10


return std::accumulate(results.begin(),results.end(),init); // 11
}
函数看起来很长,但不复杂。如果输入的范围为空1,就会得到init的值。如果范围内的元素多于一个时,需
要用范围内元素的总数量除以线程(块)中最小任务数,从而确定启动线程的最大数量2。
因为上下文频繁切换会降低线程的性能,所以计算量的最大值和硬件支持线程数,较小的值为启动线程的数
量3。 std::thread::hardware_concurrency() 返回0时,可以选择一个合适的数字。在本例中,我选择了"2"。
每个线程中处理的元素数量,是范围中元素的总量除以线程的个数得出的4,分配是否得当会在后面讨论。
现在,确定了线程个数,创建一个 std::vector<T> 容器存放中间结果,并为线程创建一
个 std::vector<std::thread> 容器5。因为在启动之前已经有了一个线程(主线程),所以启动的线程数必须比
num_threads少1。
使用循环来启动线程:block_end迭代器指向当前块的末尾6,并启动一个新线程为当前块累加结果7。当迭
代器指向当前块的末尾时,启动下一个块8。
启动所有线程后,9中的线程会处理最终块的结果。因为知道最终块是哪一个,所以最终块中有多少个元素
就无所谓了。
累加最终块的结果后,可等待 std::for_each 10创建线程(如同在代码2.8中做的那样),之后使
用 std::accumulate 将所有结果进行累加11。
结束这个例子之前,需要明确:T类型的加法不满足结合律(比如,对于float型或double型,在进行加法操作
时,系统很可能会做截断操作),因为对范围中元素的分组,会导致parallel_accumulate得到的结果可能
与 std::accumulate 的结果不同。同样的,这里对迭代器的要求更加严格:必须是前向迭代器。对于results容
器,需要保证T有默认构造函数。可以需要根据算法本身的特性,选择不同的并行方式。算法并行会在第8章
更加深入的进行讨论,并在第10章中会介绍C++17中支持的并行算法(其中 std::reduce 操作等价于这里的
parallel_accumulate)。因为不能直接从一个线程中返回值,所以需要传递results容器的引用到线程中去。另
一个办法,通过地址来获取线程执行的结果(第4章中,我们将使用future完成这种方案)。
当线程运行时,所有必要的信息都需要传入到线程中去,包括存储计算结果的位置。有时候可以传递一个标
识数,例如代码2.8中的i。不过,需要标识的函数在调用栈的底层,同时其他线程也可调用该函数,那么标识
数就会变成累赘。好消息是在设计C++的线程库时,就有预见了这种情况,实现中给每个线程附加了唯一标
识符。


2.5 线程标识
线程标识为 std::thread::id 类型,可以通过两种方式进行检索。第一种,可以通过调用 std::thread 对象的成
员函数 get_id() 来直接获取。如果 std::thread 对象没有与任何执行线程相关联, get_id() 将返
回 std::thread::type 默认构造值,这个值表示“无线程”。第二种,当前线程中调
用 std::this_thread::get_id() (这个函数定义在 <thread> 头文件中)也可以获得线程标识。
std::thread::id 对象可以自由的拷贝和对比,因为标识符可以复用。如果两个对象的 std::thread::id 相等,
那就是同一个线程,或者都“无线程”。如果不等,那么就代表了两个不同线程,或者一个有线程,另一没有线
程。
C++线程库不会限制你去检查线程标识是否一样, std::thread::id 类型对象提供了相当丰富的对比操作。比
如,为不同的值进行排序。这意味着开发者可以将其当做为容器的键值做排序,或做其他比较。按默认顺序
比较不同的 std::thread::id :当 a<b , b<c 时,得 a<c ,等等。标准库也提供 std::hash<std::thread::id> 容
器, std::thread::id 也可以作为无序容器的键值。
std::thread::id 实例常用作检测线程是否需要进行一些操作。比如:当用线程来分割一项工作(如代码2.9),
主线程可能要做一些与其他线程不同的工作,启动其他线程前,可以通过 std::this_thread::get_id() 得到自己
的线程ID。每个线程都要检查一下,其拥有的线程ID是否与初始线程的ID相同。
std::thread::id master_thread;
void some_core_part_of_algorithm()
{
if(std::this_thread::get_id()==master_thread)
{
do_master_thread_work();
}
do_common_work();
}
另外,当前线程的 std::thread::id 将存储到数据结构中。之后这个结构体对当前线程的ID与存储的线程ID做
对比,来决定操作是“允许”,还是“需要”(permitted/required)。
同样,作为线程和本地存储不适配的替代方案,线程ID在容器中可作为键值。例如,容器可以存储其掌控下
每个线程的信息,或在多个线程中互传信息。
std::thread::id 可以作为线程的通用标识符,当标识符只与语义相关(比如,数组的索引)时,就需要这个方案
了。也可以使用输出流( std::cout )来记录一个 std::thread::id 对象的值。
std::cout<<std::this_thread::get_id();
具体的输出结果是严格依赖于具体实现的,C++标准的要求就是保证ID相同的线程必须有相同的输出。


2.6 本章总结
本章讨论了C++标准库中线程的管理方式:启动线程,等待结束和不等待结束。并了解应该如何在线程启动
前,向线程函数中传递参数,如何转移线程的所有权,如何使用线程组来分割任务。
最后,讨论了使用线程标识来确定关联数据,以及特殊线程的特殊解决方案。虽然,现在已经可以依赖线
程,使用独立的数据,做独立的任务,但在某些情况下,线程间确实需要有共享数据。
第3章会讨论共享数据和线程的直接关系。
第4章会讨论在有/没有共享数据情况下的线程同步。


第3章 共享数据
本章主要内容
共享数据的问题
使用互斥保护数据
保护数据的替代方案
上一章中已经对线程管理有所了解,现在来看一下“共享数据的那些事儿”。
试想,你和朋友合租一个公寓,公寓中只有一个厨房和一个卫生间。当你的朋友在卫生间时,你就会不能使
用卫生间了。同样的问题也会出现在厨房,假如:厨房里有一个烤箱,烤香肠的同时,也在做蛋糕,那就可
能得到不想要的食物(香肠味的蛋糕)。此外,在公共空间将一件事做到一半时,发现某些需要的东西被别人拿
走,或是离开的一段时间内有些东西变动了地方,这都会令我们不爽。
同样的问题也困扰着线程。当线程访问共享数据时,必须定一些规则来限定线程可访问的数据。一个线程更
新了共享数据,需要对其他线程进行通知。从易用性的角度,同一进程中的多个线程进行数据共享有利有
弊,错误的共享数据是产生bug的主要原因。
本章就以数据共享为主题,避免上述及潜在问题的发生的同时,将共享数据的优势发挥到最大。


3.1 共享数据的问题
涉及到共享数据时,问题就是因为共享数据的修改所导致。如果共享数据只读,那么不会影响到数据,更不
会对数据进行修改,所有线程都会获得同样的数据。但当一个或多个线程要修改共享数据时,就会产生很多
麻烦。这种情况下,需要小心谨慎,才能确保所有线程都正常工作。
不变量(invariants)的概念对开发者们编写的程序会有一定的帮助——对于特殊结构体的描述,比如:“变量包
含列表中的项数”。更新通常会破坏不变量,特别是复杂的数据结构。
双链表中每个节点都有一个指针指向列表中下一个节点,还有一个指针指向前一个节点。其中不变量就是节
点A中指向“下一个”节点B的指针,还有前向指针。为了从列表中删除一个节点,其两边节点的指针都需要更
新。当其中一边更新完成时,就破坏了不变量,直到另一边也完成更新。在两边都完成更新后,不变量就稳
定了。
从一个列表中删除一个节点的步骤如下(如图3.1)
a. 找到要删除的节点N
b. 更新前一个节点指向N的指针,让这个指针指向N的下一个节点
c. 更新后一个节点指向N的指针,让这个指正指向N的前一个节点
d. 删除节点N


图3.1 从一个双链表中删除一个节点
图中b和c在相同方向上的指向和原来已经不一致了,这就破坏了不变量。
线程间的问题在于修改共享数据,会使不变量遭到破坏。删除过程中不确定是否有其他线程能够进行访问,
可能就有线程访问到刚刚删除一边的节点。这样破坏了不变量,线程就读取到要删除节点的数据(因为一边的
连接被修改,如图3.1(b))。破坏不变量的后果是不确定的,当其他线程按从左到右的顺序访问列表时,将跳
过被删除的节点。如有第二个线程尝试删除图中右边的节点,可能会让数据结构产生永久性的损坏,并使程
序崩溃。这就是并行中常见错误:条件竞争(race condition)。
3.1.1 条件竞争


假如你去一家大电影院买电影票,有很多收银台,很多人可以同时买票。当另一个收银台也在卖你想看电影
的电影票时,你的座位选择范围取决于在之前已预定的座位。当只有少量的座位剩下,就可能是一场抢票比
赛,看谁能抢到最后一张票。这就是一个条件竞争的例子:你的座位(或者电影票)都取决于购买的顺序。
并发中的竞争条件,取决于一个以上线程的执行顺序,每个线程都抢着完成自己的任务。大多数情况下,即
使改变执行顺序,也是良性竞争,结果是可以接受的。例如,两个线程同时向一个处理队列中添加任务,因
为不变量保持不变,所以谁先谁后都不会有什么影响。
当不变量遭到破坏时,才会产生条件竞争,比如:双向链表的例子。并发中对数据的条件竞争通常表示为恶
性竞争(我们对不产生问题的良性条件竞争不感兴趣)。C++标准中也定义了数据竞争这个术语,一种特殊的条
件竞争:并发的去修改一个独立对象(参见5.1.2节),数据竞争是未定义行为的起因。
恶性条件竞争通常发生于对多个数据块的修改,例如:对两个连接指针的修改(如图3.1)。操作要访问两个独
立的数据块,独立的指令会对数据块将进行修改,并且其中一个线程可能正在进行修改,另一个线程就对数
据块进行了访问。因为出现的概率低,很难查找,也很难复现。如CPU指令连续修改完成后,即使数据结构
可以让其他并发线程访问,问题再次复现的几率也相当低。当系统负载增加时,随着执行数量的增加,执行
序列问题复现的概率也在增加,这样的问题可能会出现在负载比较大的情况下。条件竞争通常是时间敏感
的,所以程序以调试模式运行时,错误常会完全消失,因为调试模式会影响程序的执行时间(即使影响不多)。
当你以写多线程程序为生,条件竞争就会成为你的梦魇。编写软件时,我们会使用大量复杂的操作,来避免
恶性条件竞争。
3.1.2 避免恶性条件竞争
这里提供一些方法来解决恶性条件竞争,最简单的办法就是对数据结构采用某种保护机制,确保只有修改线
程才能看到不变量的中间状态。从其他访问线程的角度来看,修改不是已经完成了,就是还没开始。C++标
准库提供很多类似的机制,下面会逐一介绍。
另一个选择是对数据结构和不变量进行修改,修改完的结构必须能完成一系列不可分割的变化,也就保证了
每个不变量的状态,这就是所谓的无锁编程。不过,这种方式很难得到正确的结果。到这个级别,无论是内
存模型上的细微差异,还是线程访问数据的能力,都会让工作量变的很大。
另一种处理条件竞争的方式,是使用事务的方式去处理数据结构的更新(这里的"处理"就如同对数据库进行更
新一样)。所需的一些数据和读取都存储在事务日志中,然后将之前的操作进行合并,再进行提交。当数据结
构被另一个线程修改后,或处理已经重启的情况下,提交就会无法进行,这称作为“软件事务内存”(software
transactional memory (STM)),这是一个很热门的理论研究领域。这个概念将不会在本书中再进行介绍,因
为在C++中没有对STM进行直接支持(尽管C++有事务性内存扩展的技术规范[1])。
保护共享数据结构的最基本的方式,使用C++标准库提供的互斥量。
[1] SO/IEC TS 19841:2015—Technical Specification for C++ Extensions for Transactional Memory
http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=66343 .


3.2 使用互斥量
你肯定不想让共享数据陷入条件竞争,或是出现破坏不变量的情况。将所有访问共享数据的代码标记为互斥
是否是一种更好的办法呢?这样,任何一个线程在执行时,其他线程就必须进行等待。除非该线程在修改共
享数据,否则任何线程都不可能会看到不变量的中间状态。
访问共享数据前,将数据锁住,在访问结束后,再将数据解锁。线程库需要保证,当线程使用互斥量锁住共
享数据时,其他的线程都必须等到之前那个线程对数据进行解锁后,才能进行访问数据。
互斥量是C++保护数据最通用的机制,但也需要编排代码来保护数据的正确性(见3.2.2节),并避免接口间的条
件竞争(见3.2.3节)也非常重要。不过,互斥量也会造成死锁(见3.2.4节),或对数据保护的太多(或太少)(见
3.2.8节)。
3.2.1 互斥量
通过实例化 std::mutex 创建互斥量实例,成员函数lock()可对互斥量上锁,unlock()为解锁。不过,不推荐直
接去调用成员函数,调用成员函数就意味着,必须在每个函数出口都要去调用unlock()(包括异常的情况)。
C++标准库为互斥量提供了RAII模板类 std::lock_guard ,在构造时就能提供已锁的互斥量,并在析构时进行解
锁,从而保证了互斥量能被正确解锁。下面的代码中,展示了如何在多线程应用中,使用 std::mutex 构造
的 std::lock_guard 实例,对列表进行访问保护。( std::mutex 和 std::lock_guard 都在 <mutex> 头文件中声明。)
代码3.1 使用互斥量保护列表
#include <list>
#include <mutex>
#include <algorithm>
std::list<int> some_list; // 1
std::mutex some_mutex; // 2
void add_to_list(int new_value)
{
std::lock_guard<std::mutex> guard(some_mutex); // 3
some_list.push_back(new_value);
}
bool list_contains(int value_to_find)
{
std::lock_guard<std::mutex> guard(some_mutex); // 4
return std::find(some_list.begin(),some_list.end(),value_to_find) !=
some_list.end();
}
代码3.1中有一个全局变量1,这个全局变量被一个全局的互斥量保护2。add_to_list()3和list_contains()4函
数中使用 std::lock_guard<std::mutex> ,使得这两个函数中对数据的访问是互斥的:list_contains()不可能看到
正在被add_to_list()修改的列表。


C++17中添加了一个新特性,称为模板类参数推导,类似 std::lock_guard 这样简单的模板类型,其模板参数
列表可以省略。3和4的代码可以简化成:
std::lock_guard guard(some_mutex);
具体的模板参数类型推导则交给C++17的编译器完成。3.2.4节中,会介绍C++17中的一种加强版数据保护机
制—— std::scoped_lock ,所以在C++17的环境下,上面的这行代码也可以写成:
std::scoped_lock guard(some_mutex);
为了让代码更加清晰,并且兼容只支持C++11标准的编译器,我会继续使用 std::lock_guard ,并在代码中写
明模板参数的类型。
某些情况下使用全局变量没问题,但大多数情况下,互斥量通常会与需要保护的数据放在同一类中,而不是
定义成全局变量。这是面向对象设计的准则:将其放在一个类中,就可让他们联系在一起,也可对类的功能
进行封装,并进行数据保护。这种情况下,函数add_to_list和list_contains可以作为这个类的成员函数。互斥
量和需要保护的数据,在类中都定义为private成员,这会让代码更清晰,并且方便了解什么时候对互斥量上
锁。所有成员函数都会在调用时对数据上锁,结束时对数据解锁,这就保证了访问时数据不变量的状态稳
定。
当然,也不是总能那么理想:当其中一个成员函数返回的是保护数据的指针或引用时,也会破坏数据。具有
访问能力的指针或引用可以访问(并可能修改)保护数据,而不会被互斥锁限制。这就需要对接口谨慎设计,要
确保互斥量能锁住数据访问,并且不留后门。
3.2.2 保护共享数据
使用互斥量来保护数据,并不是在每一个成员函数中加入一个 std::lock_guard 对象那么简单。一个指针或引
用,也会让这种保护形同虚设。不过,检查指针或引用很容易,只要没有成员函数通过返回值或者输出参数
的形式,向其调用者返回指向受保护数据的指针或引用,数据就是安全的。确保成员函数不会传出指针或引
用的同时,检查成员函数是否通过指针或引用的方式来调用也是很重要的(尤其是这个操作不在你的控制下
时)。函数可能没在互斥量保护的区域内存储指针或引用,这样就很危险。更危险的是:将保护数据作为一个
运行时参数,如同下面代码中所示。
代码3.2 无意中传递了保护数据的引用


class some_data
{
int a;
std::string b;
public:
void do_something();
};
class data_wrapper
{
private:
some_data data;
std::mutex m;
public:
template<typename Function>
void process_data(Function func)
{
std::lock_guard<std::mutex> l(m);
func(data); // 1 传递“保护”数据给用户函数
}
};
some_data* unprotected;
void malicious_function(some_data& protected_data)
{
unprotected=&protected_data;
}
data_wrapper x;
void foo()
{
x.process_data(malicious_function); // 2 传递一个恶意函数
unprotected->do_something(); // 3 在无保护的情况下访问保护数据
}
例子中process_data看起来没有问题, std::lock_guard 对数据做了很好的保护,但调用用户提供的函数
func1,就意味着foo能够绕过保护机制将函数 malicious_function 传递进去2,可以在没有锁定互斥量的情况
下调用 do_something() 。
这段代码的问题在于根本没有保护,只是将所有可访问的数据结构代码标记为互斥。函数 foo() 中调
用 unprotected->do_something() 的代码未能被标记为互斥。这种情况下,C++无法提供任何帮助,只能由开发者
使用正确的互斥锁来保护数据。从乐观的角度上看,还是有方法的:切勿将受保护数据的指针或引用传递到
互斥锁作用域之外。
虽然,这是使用互斥量保护共享数据时常犯的错误,但绝不仅仅是一个潜在的陷阱。下一节中,即便是使用
了互斥量对数据进行保护,条件竞争依旧存在。


3.2.3 接口间的条件竞争
使用了互斥量或其他机制保护了共享数据,就不必再为条件竞争所担忧吗?并不是,依旧需要确定数据是否
受到了保护。回想之前双链表的例子,为了能让线程安全地删除一个节点,需要确保防止对这三个节点(待删
除的节点及其前后相邻的节点)的并发访问。如果只对指向每个节点的指针进行访问保护,那就和没有使用互
斥量一样,条件竞争仍会发生——除了指针,整个数据结构和整个删除操作需要保护。这种情况下最简单的
解决方案就是使用互斥量来保护整个链表,如代码3.1所示。
尽管链表的个别操作是安全的,但依旧可能遇到条件竞争。例如,构建一个类似于 std::stack 的栈(代码
3.3),除了构造函数和swap()以外,需要对 std::stack 提供五个操作:push()一个新元素进栈,pop()一个元素
出栈,top()查看栈顶元素,empty()判断栈是否是空栈,size()了解栈中有多少个元素。即使修改了top(),返
回一个拷贝而非引用(即遵循了3.2.2节的准则),这个接口仍存在条件竞争。这个问题不仅存在于互斥量实现
接口中,在无锁实现接口中,也会产生条件竞争。这是接口的问题,与实现方式无关。
代码3.3 std::stack 容器的实现
template<typename T,typename Container=std::deque<T> >
class stack
{
public:
explicit stack(const Container&);
explicit stack(Container&& = Container());
template <class Alloc> explicit stack(const Alloc&);
template <class Alloc> stack(const Container&, const Alloc&);
template <class Alloc> stack(Container&&, const Alloc&);
template <class Alloc> stack(stack&&, const Alloc&);
bool empty() const;
size_t size() const;
T& top();
T const& top() const;
void push(T const&);
void push(T&&);
void pop();
void swap(stack&&);
template <class... Args> void emplace(Args&&... args); // C++14的新特性
};
虽然empty()和size()可能在返回时是正确的,但结果不可靠。当返回后,其他线程就可以自由地访问栈,并且
可能push()多个新元素到栈中,也可能pop()一些已在栈中的元素。这样的话,之前从empty()和size()得到的
数值就有问题了。
非共享的栈对象,如果栈非空,使用empty()检查再调用top()访问栈顶部的元素是安全的。如下代码所示:


stack<int> s;
if (! s.empty()){ // 1
int const value = s.top(); // 2
s.pop(); // 3
do_something(value);
}
不仅在单线程代码中安全,而且在空堆栈上调用top()是未定义的行为也符合预期。对于共享的栈对象,这样
的调用顺序就不再安全,因为在调用empty()1和调用top()2之间,可能有来自另一个线程的pop()调用并删除
了最后一个元素。这是一个经典的条件竞争,使用互斥量对栈内部数据进行保护,但依旧不能阻止条件竞争
的发生,这就是接口固有的问题。
怎么解决呢?问题发生在接口设计上,所以解决的方法就是变更接口设计。怎么改?这个简单的例子中调用
top()时,发现栈已经是空,就抛出异常。这能直接解决这个问题,但这是一个笨拙的解决方案,这样的话,
即使empty()返回false的情况下,也需要进行异常捕获。本质上,这会让empty()成为一个多余函数。
仔细的观察之前的代码段,在调用top()2和pop()3之间会发现另一个潜在的条件竞争。假设两个线程运行着
前面的代码,并且都引用同一个栈对象。当为性能而使用线程时,多个线程在不同的数据上执行相同的操作
很正常,并且共享栈可以将工作进行分摊。假设,一开始栈中只有两个元素,这时任一线程上的empty()和
top()都存在竞争,只需要考虑可能的执行顺序即可。
内部互斥量保护栈时,只有一个线程可以调用栈的成员函数,所以调用可以很好地交错,并且
do_something()是可以并发运行的。在表3.1中,展示一种可能的执行顺序。
表3.1 一种可能执行顺序
Thread A Thread B
if (!s.empty);
if(!s.empty);
int const value = s.top();
int const value = s.top();
s.pop();
do_something(value); s.pop();
do_something(value);
当线程运行时,调用两次top(),没修改栈,所以每个线程能得到同样的值。不仅是这样,调用top()的过程中
(两次),都没有调用pop()函数。这样,在其中一个值再读取的时候,虽然不会出现“写后读”的情况,但其值已
处理了两次。这种条件竞争,比未定义的empty()/top()竞争更加严重。虽然结果依赖于do_something()的结
果,但因为看起来没有任何错误,就会让这个Bug更难定位。
这就需要接口设计上有较大的改动,提议之一就是使用同一互斥量来保护top()和pop()。Tom Cargill[1]指出当
拷贝构造函数在栈中抛出一个异常,这样的处理方式就会有问题。在Herb Sutter[2]看来,这个问题可以从“异
常安全”的角度完美解决,不过潜在的条件竞争,可能会组成一些新的组合。
说一些大家没有意识到的问题:假设有一个 stack<vector<int>> ,vector是一个动态容器,当拷贝一个vector,
标准库会从堆上分配很多内存来完成这次拷贝。当这个系统处在重度负荷,或有严重的资源限制的情况下,
这种内存分配就会失败,所以vector的拷贝构造函数可能会抛出一个 std::bad_alloc 异常。当vector中存有大
量元素时,这种情况发生的可能性更大。当pop()函数返回“弹出值”时(也就是从栈中将这个值移除),会有一个


潜在的问题:这个值返回到调用函数的时候,栈才被改变。但拷贝数据的时候,调用函数抛出一个异常会怎
么样? 如果真的发生了,要弹出的数据将会丢失,它的确从栈上移出了,但是拷贝失败了! std::stack 的设
计人员将这个操作分为两部分:先获取顶部元素(top()),然后从栈中移除(pop())。这样,在不能安全的将元素
拷贝出去的情况下,栈中的这个数据还依旧存在,没有丢失。当问题是堆空间不足,应用可能会释放一些内
存,然后再进行尝试。
不幸的是,这样的分割却制造了本想避免的条件竞争。幸运的是,我们还有的别的选项,但使用每个选项都
有相应的代价。
选项1: 传入一个引用
第一个选项是将变量的引用作为参数,传入pop()函数中获取“弹出值”:
std::vector<int> result;
some_stack.pop(result);
这种方式还不错,缺点也很明显:需要构造出一个栈中类型的实例,用于接收目标值。对于一些类型,这样
做是不现实的,因为临时构造一个实例,从时间和资源的角度上来看都不划算。对于其他的类型,这样也不
总行得通,因为构造函数需要的参数,在这个阶段不一定可用。最后,需要可赋值的存储类型,这是一个重
大限制:即使支持移动构造,甚至是拷贝构造(从而允许返回一个值),很多用户自定义类型可能都不支持赋值
操作。
选 项 2: 无 异 常 抛 出 的 拷 贝 构 造 函 数 或 移 动 构 造 函 数
对于有返回值的pop()函数来说,只有“异常安全”方面的担忧(当返回值时可以抛出一个异常)。很多类型都有拷
贝构造函数,它们不会抛出异常,并且随着新标准中对“右值引用”的支持(详见附录A,A.1节),很多类型都将
会有一个移动构造函数,即使他们和拷贝构造函数做着相同的事情,也不会抛出异常。一个有用的选项可以
限制对线程安全栈的使用,并且能让栈安全的返回所需的值,而不抛出异常。
虽然安全,但非可靠。尽管能在编译时可使
用 std::is_nothrow_copy_constructible 和 std::is_nothrow_move_constructible ,让拷贝或移动构造函数不抛出异
常,但是这种方式的局限性太强。用户自定义的类型中,会有不抛出异常的拷贝构造函数或移动构造函数的
类型, 那些有抛出异常的拷贝构造函数,但没有移动构造函数的类型往往更多(这种情况会随着人们习惯于
C++11中的右值引用而有所改变)。如果这些类型不能存储在线程安全的栈中,那将是多么的不幸。
选 项 3: 返 回 指 向 弹 出 值 的 指 针
第三个选择是返回一个指向弹出元素的指针,而不是直接返回值。指针的优势是自由拷贝,并且不会产生异
常,这样就能避免Cargill提到的异常问题了。缺点就是返回指针需要对对象的内存分配进行管理,对于简单数
据类型(比如:int),内存管理的开销要远大于直接返回值。对于这个方案,使用 std::shared_ptr 是个不错的选
择,不仅能避免内存泄露(因为当对象中指针销毁时,对象也会被销毁),而且标准库能够完全控制内存分配方
案,就不需要new和delete操作。这种优化是很重要的:因为堆栈中的每个对象,都需要用new进行独立的内
存分配,相较于非线程安全版本,这个方案的开销相当大。
选项4:“选项1 + 选项2”或 “选项1 + 选项3”
对于通用的代码来说,灵活性不应忽视。当已经选择了选项2或3时,再去选择1也是很容易的。这些选项提供
给用户,让用户自己选择最合适,最经济的方案。
例:定义线程安全的堆栈
代码3.4中是一个接口没有条件竞争的堆栈类定义,它实现了选项1和选项3:重载了pop(),使用局部引用去存
储弹出值,并返回 std::shared_ptr<> 对象。它有一个简单的接口,只有两个函数:push()和pop();
代码3.4 线程安全的堆栈类定义(概述)


#include <exception>
#include <memory> // For std::shared_ptr<>
struct empty_stack: std::exception
{
const char* what() const throw();
};
template<typename T>
class threadsafe_stack
{
public:
threadsafe_stack();
threadsafe_stack(const threadsafe_stack&);
threadsafe_stack& operator=(const threadsafe_stack&) = delete; // 1 赋值操
作被删除
void push(T new_value);
std::shared_ptr<T> pop();
void pop(T& value);
bool empty() const;
};
削减接口可以获得最大程度的安全,甚至限制对栈的一些操作。栈是不能直接赋值的,因为赋值操作已经删除
了1(详见附录A,A.2节),并且这里没有swap()函数。当栈为空时,pop()函数会抛出一个empty_stack异常,
所以在empty()函数被调用后,其他部件还能正常工作。如选项3描述的那样,使用 std::shared_ptr 可以避免
内存分配管理的问题,并避免多次使用new和delete操作。堆栈中的五个操作,现在就剩下三个:push(),
pop()和empty()(这里empty()都有些多余)。简化接口更有利于数据控制,可以保证互斥量将操作完全锁住。下
面的代码展示了一个简单的实现——封装 std::stack<> 的线程安全堆栈。
代码3.5 扩充(线程安全)堆栈


#include <exception>
#include <memory>
#include <mutex>
#include <stack>
struct empty_stack: std::exception
{
const char* what() const throw() {
return "empty stack!";
};
};
template<typename T>
class threadsafe_stack
{
private:
std::stack<T> data;
mutable std::mutex m;
public:
threadsafe_stack()
: data(std::stack<T>()){}
threadsafe_stack(const threadsafe_stack& other)
{
std::lock_guard<std::mutex> lock(other.m);
data = other.data; // 1 在构造函数体中的执行拷贝
}
threadsafe_stack& operator=(const threadsafe_stack&) = delete;
void push(T new_value)
{
std::lock_guard<std::mutex> lock(m);
data.push(new_value);
}
std::shared_ptr<T> pop()
{
std::lock_guard<std::mutex> lock(m);
if(data.empty()) throw empty_stack(); // 在调用pop前,检查栈是否为空
std::shared_ptr<T> const res(std::make_shared<T>(data.top())); // 在修改
堆栈前,分配出返回值
data.pop();
return res;
}


void pop(T& value)
{
std::lock_guard<std::mutex> lock(m);
if(data.empty()) throw empty_stack();
value=data.top();
data.pop();
}
bool empty() const
{
std::lock_guard<std::mutex> lock(m);
return data.empty();
}
};
堆栈可以拷贝——拷贝构造函数对互斥量上锁,再拷贝堆栈。构造函数体中1的拷贝使用互斥量来确保复制
结果的正确性,这样的方式比成员初始化列表好。
之前对top()和pop()函数的讨论中,因为锁的粒度太小,恶性条件竞争已经出现,需要保护的操作并未全覆盖
到。不过,锁的颗粒度过大同样会有问题。还有一个问题,一个全局互斥量要去保护全部共享数据,在一个
系统中存在有大量的共享数据时,线程可以强制运行,甚至可以访问不同位置的数据,抵消了并发带来的性
能提升。第一版为多处理器系统设计Linux内核中,就使用了一个全局内核锁。这个锁能正常工作,但在双核
处理系统的上的性能要比两个单核系统的性能差很多,四核系统就更不能提了。太多请求去竞争占用内核,
使得依赖于处理器运行的线程没有办法很好的工作。随后修正的Linux内核加入了一个细粒度锁方案,因为少
了很多内核竞争,这时四核处理系统的性能就和单核处理的四倍差不多了。
使用多个互斥量保护所有的数据,细粒度锁也有问题。如前所述,当增大互斥量覆盖数据的粒度时,只需要
锁住一个互斥量。但这种方案并非放之四海皆准,互斥量保护一个独立类的实例,锁的状态的下一个阶段,
不是离开锁定区域将锁定区域还给用户,就是有独立的互斥量去保护这个类的全部实例,两种方式都不怎么
好。
一个给定操作需要两个或两个以上的互斥量时,另一个潜在的问题将出现:死锁。与条件竞争完全相反—
不同的两个线程会互相等待,从而什么都没做。
3.2.4 死锁:问题描述及解决方案
试想有一个玩具,这个玩具由两部分组成,必须拿到这两个部分,才能够玩。例如玩具鼓,需要鼓锤和鼓才
能玩。有两个小孩,他们都很喜欢玩这个玩具。当其中一个孩子拿到了鼓和鼓锤时,那就可以尽情的玩耍
了。当另一孩子想要玩,他就得等待另一孩子玩完才行。再试想,鼓和鼓锤被放在不同的玩具箱里,并且两
个孩子在同一时间里都想要去敲鼓。之后,他们就去玩具箱里面找这个鼓。其中一个找到了鼓,并且另外一
个找到了鼓锤。现在问题就来了,除非其中一个孩子决定让另一个先玩,他可以把自己的那部分给另外一个
孩子。但当他们都紧握着自己所有的部分,那么这个鼓谁都没法玩。
现在没有孩子去争抢玩具,但线程有对锁的竞争:一对线程需要对他们所有的互斥量做一些操作,其中每个
线程都有一个互斥量,且等待另一个解锁。因为他们都在等待对方释放互斥量,没有线程能工作。这种情况
就是死锁,它的问题就是由两个或两个以上的互斥量进行锁定。


避免死锁的一般建议,就是让两个互斥量以相同的顺序上锁:总在互斥量B之前锁住互斥量A,就永远不会死
锁。某些情况下是可以这样用,因为不同的互斥量用于不同的地方。不过,当有多个互斥量保护同一个类的
独立实例时,一个操作对同一个类的两个不同实例进行数据的交换操作,为了保证数据交换操作的正确性,
就要避免并发修改数据,并确保每个实例上的互斥量都能锁住自己要保护的区域。不过,选择一个固定的顺
序(例如,实例提供的第一互斥量作为第一个参数,提供的第二个互斥量为第二个参数),可能会适得其反:在
参数交换了之后,两个线程试图在相同的两个实例间进行数据交换时,程序又死锁了!
很幸运,C++标准库有办法解决这个问题, std::lock ——可以一次性锁住多个(两个以上)的互斥量,并且没
有副作用(死锁风险)。下面的程序代码中,就来看一下怎么在一个简单的交换操作中使用 std::lock 。
代码3.6 交换操作中使用 std::lock() 和 std::lock_guard
// 这里的std::lock()需要包含<mutex>头文件
class some_big_object;
void swap(some_big_object& lhs,some_big_object& rhs);
class X
{
private:
some_big_object some_detail;
std::mutex m;
public:
X(some_big_object const& sd):some_detail(sd){}
friend void swap(X& lhs, X& rhs)
{
if(&lhs==&rhs)
return;
std::lock(lhs.m,rhs.m); // 1
std::lock_guard<std::mutex> lock_a(lhs.m,std::adopt_lock); // 2
std::lock_guard<std::mutex> lock_b(rhs.m,std::adopt_lock); // 3
swap(lhs.some_detail,rhs.some_detail);
}
};
首先检查参数,因为操作试图获取 std::mutex 对象上的锁,所以结果很难预料。(互斥量可以在同一线程上多
次上锁,标准库中 std::recursive_mutex 提供这样的功能。详情见3.3.3节)。然后,调用 std::lock() 1锁住两
个互斥量,并且创建两个 std:lock_guard 实例23。提供std::adopt_lock 参数除了表示 std::lock_guard 可获取
锁之外,还将锁交由 std::lock_guard 管理,就不需要 std::lock_guard 再去构建新的锁了。
这样,就能保证在大多数情况下,函数退出时互斥量能解锁(保护操作可能会抛出一个异常),也允许使用一个
简单的“return”作为返回。当使用 std::lock 去锁lhs.m或rhs.m时,可能会抛出异常,异常会传播
到 std::lock 之外。当 std::lock 获取互斥锁时,并尝试从另一个互斥量上再获取锁时,就会有异常抛出,第
一个锁也会随着异常而自动释放,所以 std::lock 要么将两个锁都锁住,要不一个都不锁。
C++17对这种情况提供了支持, std::scoped_lock<> 是一种新的RAII模板类型,与 std::lock_guard<> 的功能相
同,这个新类型能接受不定数量的互斥量类型作为模板参数,以及相应的互斥量(数量和类型)作为构造参数。
互斥量支持构造时上锁,与 std::lock 的用法相同,解锁在析构中进行。代码3.6中swap()操作可以重写如
下:


void swap(X& lhs, X& rhs)
{
if(&lhs==&rhs)
return;
std::scoped_lock guard(lhs.m,rhs.m); // 1
swap(lhs.some_detail,rhs.some_detail);
}
这里使用了C++17的另一个特性:自动推导模板参数。如果有支持C++17的编译器(就能使
用 std::scoped_lock 了,因为其是C++17标准库中的一个工具),C++17可以通过隐式参数模板类型推导机制,
通过传递的对形象类型来构造实例1。这行代码等价于下面全给参数的版本:
std::scoped_lock<std::mutex,std::mutex> guard(lhs.m,rhs.m);
std::scoped_lock 的好处在于,可以将所有 std::lock 替换掉,从而减少错误的发生。
虽然 std::lock (和 std::scoped_lock<> )可以在这情况下(获取两个以上的锁)避免死锁,但它没办法帮助你获取
其中一个锁。这需要依赖开发者的纪律性(译者:也就是经验),来确保程序不会死锁。
死锁是多线程编程中令人相当头痛的问题,并且死锁经常是不可预见的,因为在大部分时间里,所有工作都
能很好的完成。不过,一些相对简单的规则能帮助写出“无死锁”的代码。
3.2.5 避免死锁的进阶指导
死锁通常是对锁的使用不当造成。无锁的情况下,仅需要两个线程 std::thread 对象互相调用join()就能产生死
锁。这种情况下,没有线程可以继续运行,因为他们正在互相等待。这种情况很常见,一个线程会等待另一
个线程,其他线程同时也会等待第一个线程结束,所以三个或更多线程的互相等待也会发生死锁。为了避免
死锁,这里意见:不 要 谦 让 。以下提供一些个人建议。
避免嵌套锁
第一个建议往往是最简单的:线程获得一个锁时,就别再去获取第二个。每个线程只持有一个锁,就不会产
生死锁。当需要获取多个锁,使用 std::lock 来做这件事(对获取锁的操作上锁),避免产生死锁。
避免在持有锁时调用外部代码
第二个建议是次简单的:因为代码是外部提供的,所以没有办法确定外部要做什么。外部程序可能做任何事
情,包括获取锁。在持有锁的情况下,如果用外部代码要获取一个锁,就会违反第一个指导意见,并造成死
锁(有时这是无法避免的)。当写通用代码时(例如3.2.3中的栈),每一个操作的参数类型,都是外部提供的定
义,这就需要其他指导意见来帮助你了。
使用固定顺序获取锁
当硬性要求获取两个或两个以上的锁,并且不能使用 std::lock 单独操作来获取它们时,最好在每个线程上,
用固定的顺序获取它们(锁)。3.2.4节中提到,当需要获取两个互斥量时,需要以一定的顺序获取锁。一些情
况下,这种方式相对简单。比如,3.2.3节中的栈——每个栈实例中都内置有互斥量,但是对数据成员存储的
操作上,栈就需要调用外部代码。虽然,可以添加一些约束,对栈上存储的数据项不做任何操作,但对数据
项的处理仅限于栈自身。这会让使用通用栈的难度有所增加,但是一个容器很少去访问另一个容器中存储的
数据,即使发生了也会很显眼,所以这对于通用栈来说并不是一个特别重的负担。


其他情况下,这就没那么简单了(例如:3.2.4节中的交换操作),这时可能同时锁住多个互斥量(有时不会发
生)。3.1节中那个链表连接例子中,列表中的每个节点都会有一个互斥量保护。为了访问链表,线程必须获取
感兴趣节点上的互斥锁。当一个线程删除一个节点,就必须获取三个节点上的互斥锁:将要删除的节点,两
个邻接节点。为了遍历链表,线程必须保证在获取当前节点的互斥锁前提下,获得下一个节点的锁,要保证
指向下一个节点的指针不会同时被修改。当下一个节点上的锁被获取,第一个节点的锁就可以释放了。
这种“手递手”的模式允许多个线程访问链表,为每一个访问的线程提供不同的节点。为了避免死锁,节点必须
以固定的顺序上锁:如果两个线程试图用互为反向的顺序,在使用“手递手”遍历列表时,执行到链表中间部分
时会发生死锁。当节点A和B在列表中相邻,当前线程可能会同时尝试获取A和B上的锁。另一个线程可能已经
获取了节点B上的锁,并试图获取节点A上的锁——经典的死锁场景,如图3.2所示。
线程1 线程2
锁住主入口的互斥量
读取头结点指针
锁住头结点互斥量
解锁主入口互斥量
锁住主入口互斥量
读取head->next指针 锁住尾结点互斥量
锁住next结点的互斥量 读取tail->prev指针
读取next->next指针 解锁尾结点的互斥量
... ...
锁住A结点的互斥量 锁住C结点的互斥量
读取A->next指针(也就是B结点) 读取C->next指针(也就是B结点)
锁住B结点互斥量
阻塞,尝试锁住B结点的互斥量 解锁C结点互斥量
读取B->prev指针(也就是A结点)
阻塞,尝试锁住A结点的互斥量
死锁!
图3.2 不同线程以相反顺序访问列表所造成的死锁
当A、C节点中间的B节点删除时,有线程在已获取A和C上的锁后,还要获取B节点上的锁时,就可能发生死
锁。线程可能会试图先锁住A节点或C节点(根据遍历的方向),但是发现无法获得B上的锁,因为执行删除任务
的线程,已经获取了B上的锁。
这里提供一种避免死锁的方式,定义遍历的顺序,一个线程必须先锁住A才能获取B的锁,在锁住B之后才能
获取C的锁。这将消除死锁,不允许反向遍历链表。类似的约定常用于建立其他的数据结构。
使用层次锁结构
虽然,定义锁的顺序是一种特殊情况,但层次锁的意义在于,在运行时会约定是否进行检查。这个建议需要
对应用进行分层,并且识别在给定层上所有互斥量。当代码试图对互斥量上锁,而低层已持有该层锁时,不
允许锁定。可以通过每个互斥量对应的层数,以及每个线程使用的互斥量,在运行时检查锁定操作是否可以
进行。下面的代码列表中,展示两个线程如何使用进行分层互斥的。


代码3.7 使用层次锁来避免死锁
hierarchical_mutex high_level_mutex(10000); // 1
hierarchical_mutex low_level_mutex(5000); // 2
hierarchical_mutex other_mutex(6000); // 3
int do_low_level_stuff();
int low_level_func()
{
std::lock_guard<hierarchical_mutex> lk(low_level_mutex); // 4
return do_low_level_stuff();
}
void high_level_stuff(int some_param);
void high_level_func()
{
std::lock_guard<hierarchical_mutex> lk(high_level_mutex); // 6
high_level_stuff(low_level_func()); // 5
}
void thread_a() // 7
{
high_level_func();
}
void do_other_stuff();
void other_stuff()
{
high_level_func(); // 10
do_other_stuff();
}
void thread_b() // 8
{
std::lock_guard<hierarchical_mutex> lk(other_mutex); // 9
other_stuff();
}
这段代码有三个hierarchical_mutex实例(1,2和3),其通过逐渐递减的层级进行构造。根据已经定义好的
机制,如将一个hierarchical_mutex实例进行上锁,那么只能获取更低层级实例上的锁,这就会对代码进行一
些限制。
假设do_low_level_stuff不会对任何互斥量进行上锁,low_level_func为层级最低的函数,并且会对
low_level_mutex4进行上锁。high_level_func调用low_level_func5的同时,也持有high_level_mutex6上的
锁,这也没什么问题,因为high_level_mutex(1:10000)要比low_level_mutex(2:5000)更高级。


thread_a()7遵守规则,所以运行没问题。
另一方面,thread_b()8无视规则,因此在运行时会失败。
首先,thread_b锁住了other_mutex9,这个互斥量的层级值只有60003。这就意味着,中层级的数据已被保
护。当other_stuff()调用high_level_func()8时,就违反了层级结构:high_level_func()试图获取
high_level_mutex,这个互斥量的层级值是10000,要比当前层级值6000大很多。因此hierarchical_mutex将
会产生一个错误,可能会是抛出一个异常或直接终止程序。层级互斥量不可能死锁,因为互斥量本身会严格
遵循约定进行上锁。当多个互斥量在是在同一级上时,不能同时持有多个锁,所以“手递手”的方案需要每个互
斥量在一条链上,并且每个互斥量都比前一个有更低的层级值,这在某些情况下无法实现。
例子也展示了 std::lock_guard<> 模板与用户自定义的互斥量类型如何一起使用。虽然hierarchical_mutex不是
C++标准的一部分,但是写起来很容易,代码3.8中有一个简单的实现。尽管它是一个用户定义类型,可用
于 std::lock_guard<> 模板中,为了满足互斥量操作,其有三个成员函数:lock(), unlock() 和 try_lock()。
try_lock()使用起来很简单:当互斥量上的锁被一个线程持有,它将返回false,而不是等待调用的线程,直到
能够获取互斥量上的锁为止。 std::lock() 的内部实现中,try_lock()作为避免死锁算法的一部分。
代码3.8 简单的层级互斥量实现


class hierarchical_mutex
{
std::mutex internal_mutex;
unsigned long const hierarchy_value;
unsigned long previous_hierarchy_value;
static thread_local unsigned long this_thread_hierarchy_value; // 1
void check_for_hierarchy_violation()
{
if(this_thread_hierarchy_value <= hierarchy_value) // 2
{
throw std::logic_error(“mutex hierarchy violated”);
}
}
void update_hierarchy_value()
{
previous_hierarchy_value=this_thread_hierarchy_value; // 3
this_thread_hierarchy_value=hierarchy_value;
}
public:
explicit hierarchical_mutex(unsigned long value):
hierarchy_value(value),
previous_hierarchy_value(0)
{}
void lock()
{
check_for_hierarchy_violation();
internal_mutex.lock(); // 4
update_hierarchy_value(); // 5
}
void unlock()
{
if(this_thread_hierarchy_value!=hierarchy_value)
throw std::logic_error(“mutex hierarchy violated”); // 9
this_thread_hierarchy_value=previous_hierarchy_value; // 6
internal_mutex.unlock();
}
bool try_lock()
{
check_for_hierarchy_violation();
if(!internal_mutex.try_lock()) // 7


return false;
update_hierarchy_value();
return true;
}
};
thread_local unsigned long
hierarchical_mutex::this_thread_hierarchy_value(ULONG_MAX); // 8
这里重点是使用了thread_local的值来代表当前线程的层级值:this_thread_hierarchy_value1,初始化为最
大值8,所以最初所有线程都能被锁住。因为声明中有thread_local,所以每个线程都有其副本,这样线程中
变量状态完全独立,当从另一个线程进行读取时,变量的状态也完全独立。
所以,线程第一次锁住一个hierarchical_mutex时,this_thread_hierarchy_value的值是ULONG_MAX。由于
其本身的性质,这个值会大于其他任何值,所以通过了check_for_hierarchy_vilation()2的检查。这种检查
下,lock()代表内部互斥锁已锁住4。一旦成功锁住,就可以更新层级值了5。
当持有第一个锁的同时,还锁住了另一个hierarchical_mutex,this_thread_hierarchy_value的值将会显示第
一个互斥量的层级值。第二个互斥量的层级值必须小于已持有互斥量,检查函数2才能通过。
现在,最重要的是为当前线程赋予之前的层级值,可以调用unlock()6对层级值进行保存。否则,就锁不住任
何互斥量(第二个互斥量的层级数高于第一个互斥量),即使线程没有持有任何锁。因为保存了之前的层级值,
只有当持有internal_mutex3,且在解锁内部互斥量6之前存储它的层级值时,需要内部互斥量对
hierarchical_mutex实例进行保护,才能安全的将hierarchical_mutex存储。为了避免无序解锁造成层次混乱,
不是解锁最近上锁的那个互斥量,就需要抛出异常9。其他机制也能做到这点,但目前这是最简单的。
try_lock()与lock()的功能相似,除了在调用internal_mutex的try_lock()7失败时,不能持有对应锁,所以不必
更新层级值,并直接返回false。
虽然是运行时检测,但无时间依赖性——不必去等待构成死锁的条件出现。同时,设计过程需要拆分应用,
互斥量在这种情况下可以消除死锁的可能性。这样的练习很有必要去做一下,即使你之后没有去做,代码也
会在运行时检查。
超越锁的延伸扩展
死锁不仅仅会发生在锁之间,也会发生在同步构造中(可能会产生一个等待循环),这也需要有指导意见,例
如:获取嵌套锁,等待一个持有锁的线程,都是很糟糕的决定(因为线程为了能继续运行可能需要获取对应的
锁)。如果去等待一个线程结束,应该确定这个线程的层级,这样一个线程只需要等待比其层级低的线程结束
即可。用一个简单的办法便可确定,添加的线程是否在同一函数中启动,如同在3.1.2节和3.3节中描述的那
样。
代码已能规避死锁, std::lock() 和 std::lock_guard 可组成简单的锁,并覆盖大多数情况,但有时需要更多的
灵活性,可以使用标准库提供的 std::unique_lock 模板。如 std::lock_guard ,这是一个参数化的互斥量模板
类,它提供很多RAII类型锁用来管理 std::lock_guard 类型,可以让代码更加灵活。
3.2.6 std::unique_lock——灵活的锁
std::unqiue_lock 使用起来更为自由, std::unique_lock 实例不会总与互斥量的数据类型相关,使用起来要
比 std:lock_guard 更加灵活。首先,可将 std::adopt_lock 作为第二个参数传入构造函数,对互斥量进行管理。
也可以将 std::defer_lock 作为第二个参数传递进去,表明互斥量应保持解锁状态。这样就可以
让 std::unique_lock 对象(不是互斥量)的lock()所获取,或传递 std::unique_lock 对象到 std::lock() 中。代码3.6
可以轻易的转换为代码3.9,使用 std::unique_lock 和 std::defer_lock 1,而


非 std::lock_guard 和 std::adopt_lock 。代码长度相同,几乎等价,唯一不同的就是: std::unique_lock 会占用
比较多的空间,并且比 std::lock_guard 稍慢一些。保证灵活性要付出代价,这个代价就是允
许 std::unique_lock 实例不带互斥量:信息已存储,且已更新。
代码3.9 交换操作中 std::lock() 和 std::unique_lock 的使用
class some_big_object;
void swap(some_big_object& lhs,some_big_object& rhs);
class X
{
private:
some_big_object some_detail;
std::mutex m;
public:
X(some_big_object const& sd):some_detail(sd){}
friend void swap(X& lhs, X& rhs)
{
if(&lhs==&rhs)
return;
std::unique_lock<std::mutex> lock_a(lhs.m,std::defer_lock); // 1
std::unique_lock<std::mutex> lock_b(rhs.m,std::defer_lock); // 1
std::defer_lock 留下未上锁的互斥量
std::lock(lock_a,lock_b); // 2 互斥量在这里上锁
swap(lhs.some_detail,rhs.some_detail);
}
};
代码3.9中,因为 std::unique_lock 支持lock(), try_lock()和unlock()成员函数,所以能将 std::unique_lock 对象
传递到 std::lock() 2。这些同名成员函数在低层做着实际的工作,并且仅更新std::unique_lock 实例中的标
志,来确定该实例是否拥有特定的互斥量,这个标志是为了确保unlock()在析构函数中正确调用。如果实例拥
有互斥量,那么析构函数必须调用unlock()。但当实例中没有互斥量时,析构函数就不能去调用unlock(),这
个标志可以通过owns_lock()成员变量进行查询。除非想将 std::unique_lock 的所有权进行转让,最好使用
C++17中提供的 std::scoped_lock (详见3.2.4节)。
如期望的那样,这个标志存储在了某个地方。因此, std::unique_lock 实例的体积通常要比 std::lock_guard 实
例大,当使用 std::unique_lock 替代 std::lock_guard ,会对标志进行更新或检查,就会有一些轻微的性能惩
罚。当 std::lock_guard 已经能够满足需求时,建议继续使用。当需要更加灵活的锁时,最好选
择 std::unique_lock ,因为它更适合于你的任务。我们已经看到一个递延锁的例子,另外一种情况是锁的所有
权从一个域转到另一个域。
3.2.7 不同域中互斥量的传递
std::unique_lock 实例没有与自身相关的互斥量,互斥量的所有权可以通过移动操作,在不同的实例中进行传
递。某些情况下,这种转移是自动发生的,例如:当函数返回一个实例。另一种情况下,需要显式的调
用 std::move() 来执行移动操作。本质上来说,需要依赖于源值是否是左值——一个实际的值或是引用——或
一个右值——一个临时类型。当源值是一个右值,为了避免转移所有权过程出错,就必须显式移动成左
值。 std::unique_lock 是可移动,但不可赋值的类型。


一种使用可能是允许函数去锁住一个互斥量,并且将所有权移到调用者上,所以调用者可以在这个锁保护的
范围内执行额外的动作。
下面的程序片段展示了:函数get_lock()锁住了互斥量,然后准备数据,返回锁的调用函数。
std::unique_lock<std::mutex> get_lock()
{
extern std::mutex some_mutex;
std::unique_lock<std::mutex> lk(some_mutex);
prepare_data();
return lk; // 1
}
void process_data()
{
std::unique_lock<std::mutex> lk(get_lock()); // 2
do_something();
}
lk在函数中被声明为自动变量,它不需要调用 std::move() ,可以直接返回1(编译器负责调用移动构造函数)。
process_data()函数直接转移 std::unique_lock 实例的所有权2,调用do_something()可使用的正确数据(数据
没有受到其他线程的修改)。
通常这种模式会用于已锁的互斥量,其依赖于当前程序的状态,或依赖于传入返回类型为 std::unique_lock 的
函数(或以参数返回)。这样不会直接返回锁,不过网关类的数据成员可用来确认,是否已经对保护数据的访问
权限进行上锁。这种情况下,所有的访问都必须通过网关类:当你想要访问数据,需要获取网关类的实例(如
同前面的例子,通过调用get_lock()之类函数)来获取锁。之后就可以通过网关类的成员函数对数据进行访问,
完成访问时可以销毁这个网关类对象,将锁进行释放,让别的线程来访问保护数据。这样的一个网关类可能
是可移动的(所以可以从函数进行返回),这种情况下锁对象的数据必须可移动。
std::unique_lock 的灵活性同样也允许实例在销毁之前放弃拥有的锁。可以使用unlock()来做这件事,如同一
个互斥量: std::unique_lock 的成员函数提供类似于锁定和解锁的功能。 std::unique_lock 实例有在销毁前释放
锁的能力,当没有必要在持有锁的时候,可以在特定的代码分支对锁进行选择性释放。这对于应用的性能来
说非常重要,因为持有锁的时间增加会导致性能下降,其他线程会等待这个锁的释放,避免超越操作。
3.2.8 锁的粒度
3.2.3节中,已经对锁的粒度有所了解:锁的粒度是一个华而不实的术语(hand-waving term),用来描述通过一
个锁保护着的数据量大小。一个细粒度锁(a fine-grained lock)能够保护较小的数据量,一个粗粒度锁(a
coarse-grained lock)能够保护较多的数据量。粒度对于锁来说很重要,为了保护对应的数据,保证锁有能力
保护这些数据也很重要。
在超市等待结账的时候,正在结账的顾客突然意识到忘了拿蔓越莓酱,然后离开柜台去拿,并让其他的人都
等待他回来。或者当收银员,准备收钱时,顾客才去翻钱包拿钱,这样的情况都会让等待的顾客很无奈。当
每个人都检查了自己要拿的东西,且能随时为拿到的商品进行支付时,每件事都会进行得很顺利。
道理同样适用于线程:如果很多线程正在等待同一个资源(等待收银员对自己拿到的商品进行清点),当有线程
持有锁的时间过长,这就会增加等待的时间(别等到结账的时候,才想起来蔓越莓酱没拿)。可能的情况下,锁
住互斥量的同时只能对共享数据进行访问,试图对锁外数据进行处理。特别是做一些费时的动作,比如:对


文件的输入/输出操作进行上锁。文件输入/输出通常要比从内存中读或写同样长度的数据慢成百上千倍,所以
除非锁已经打算去保护对文件的访问,要么执行输入/输出操作将会将延迟其他线程执行的时间,这没有必要
(因为文件锁阻塞住了很多操作),这样多线程带来的性能效益会被抵消。
std::unique_lock 在这种情况下工作正常,调用unlock()时,代码不需要再访问共享数据。当再次需要对共享
数据进行访问时,再调用lock()就可以了。
void get_and_process_data()
{
std::unique_lock<std::mutex> my_lock(the_mutex);
some_class data_to_process=get_next_data_chunk();
my_lock.unlock(); // 1 不要让锁住的互斥量越过process()函数的调用
result_type result=process(data_to_process);
my_lock.lock(); // 2 为了写入数据,对互斥量再次上锁
write_result(data_to_process,result);
}
不需要让锁住的互斥量越过对process()函数的调用,所以可以在函数调用1前对互斥量进行手动解锁,之后
对其再次上锁2。
这表示只有一个互斥量保护整个数据结构时的情况,不仅会有更多对锁的竞争,也会增加持锁的时长。较多
的操作步骤需要获取同一个互斥量上的锁,所以持有锁的时间会更长。成本上的双重打击也算是为了向细粒
度锁转移提供了激励和可能。
如同上面的例子,锁不仅是能锁住合适粒度的数据,还要控制锁的持有时间,以及哪些操作在执行的同时能
够拥有锁。一般情况下,尽可能将持有锁的时间缩减到最小。
代码3.6和3.9中,交换操作需要锁住两个互斥量,其明确要求并发访问两个对象。假设用来做比较的是一个简
单的数据类型(比如:int类型),将会有什么不同么?int的拷贝很廉价,所以可以进行数据复制,并且每个比较
的对象都持有该对象的锁,在比较之后进行数据拷贝。在最短时间内持有每个互斥量,并且不会在持有一个
锁的同时再去获取另一个。下面的代码中展示了这样情景中的Y类,并且展示了一个相等比较运算符的等价实
现。
代码3.10 比较操作符中一次锁住一个互斥量


class Y
{
private:
int some_detail;
mutable std::mutex m;
int get_detail() const
{
std::lock_guard<std::mutex> lock_a(m); // 1
return some_detail;
}
public:
Y(int sd):some_detail(sd){}
friend bool operator==(Y const& lhs, Y const& rhs)
{
if(&lhs==&rhs)
return true;
int const lhs_value=lhs.get_detail(); // 2
int const rhs_value=rhs.get_detail(); // 3
return lhs_value==rhs_value; // 4
}
};
例子中,比较操作符首先通过调用get_detail()成员函数检索要比较的值23,函数在索引时被锁保护着1。比
较操作符会在之后比较索引出来的值4。注意:虽然锁只持有一次的操作能减少锁持有的时间(这样能消除死
锁的可能性),但这里有一个微妙的语义操作同时对两个锁住的值进行比较。
代码3.10中,当操作符返回true时,就意味着在这个时间点上的lhs.some_detail与另一个时间点的
rhs.some_detail相同。这两个值在读取之后,可能会以任意方式修改。两个值会在2和3处进行交换,这样
就会失去了比较的意义。比较可能会返回true,表明这两个值是相等的,实际上这两个值相等的情况可能就发
生在一瞬间。这样的变化必须要小心,语义操作是无法改变比较方式的:当持有锁的时间没有达到整个操作
时间,就会让自己处于条件竞争的状态。
有时可能找不到一个合适的粒度级别,因为并不是所有对数据结构的访问都需要同一级的保护。这个例子
中,就需要寻找一个合适的机制,去替换 std::mutex 。
[1] Tom Cargill, “Exception Handling: A False Sense of Security,” in C++ Report 6, no. 9 (November
December 1994). Also available at
http://www.informit.com/content/images/020163371x/supplements/Exception_Handling_Article.html.
[2] Herb Sutter, Exceptional C++: 47 Engineering Puzzles, Programming Problems, and Solutions (Addison
Wesley Pro-fessional, 1999).


3.3 保护共享数据的方式
互斥量是一种通用的机制,但其并非保护共享数据的唯一方式。有很多方式可以在特定情况下,对共享数据
提供合适的保护。
一个特别极端的情况就是,共享数据在并发访问和初始化时(都需要保护),需要进行隐式同步。这可能是因为
数据作为只读方式创建,所以没有同步问题,或者因为必要的保护作为对数据操作的一部分。任何情况下,
数据初始化后锁住一个互斥量,纯粹是为了保护其初始化过程,并且会给性能带来不必要的影响。
出于以上的原因,C++标准提供了一种纯粹保护共享数据初始化过程的机制。
3.3.1 保护共享数据的初始化过程
假设有一个共享源,构建代价很昂贵,它可能会打开一个数据库连接或分配出很多的内存。
延迟初始化(Lazy initialization)在单线程代码很常见——每一个操作都需要先对源进行检查,为了了解数据是
否被初始化,然后在其使用前决定,数据是否需要初始化:
std::shared_ptr<some_resource> resource_ptr;
void foo()
{
if(!resource_ptr)
{
resource_ptr.reset(new some_resource); // 1
}
resource_ptr->do_something();
}
转为多线程代码时,只有1处需要保护,这样共享数据对于并发访问就是安全的。但是下面天真的转换会使
得线程资源产生不必要的序列化,为了确定数据源已经初始化,每个线程必须等待互斥量。
代码3.11 使用延迟初始化(线程安全)的过程
std::shared_ptr<some_resource> resource_ptr;
std::mutex resource_mutex;
void foo()
{
std::unique_lock<std::mutex> lk(resource_mutex); // 所有线程在此序列化
if(!resource_ptr)
{
resource_ptr.reset(new some_resource); // 只有初始化过程需要保护
}
lk.unlock();
resource_ptr->do_something();
}


这段代码相当常见了,也足够表现出没必要的线程化问题,很多人能想出更好的一些的办法来做这件事,包
括声名狼藉的“双重检查锁模式”:
void undefined_behaviour_with_double_checked_locking()
{
if(!resource_ptr) // 1
{
std::lock_guard<std::mutex> lk(resource_mutex);
if(!resource_ptr) // 2
{
resource_ptr.reset(new some_resource); // 3
}
}
resource_ptr->do_something(); // 4
}
指针第一次读取数据不需要获取锁1,并且只有在指针为空时才需要获取锁。然后,当获取锁之后,会再检
查一次指针2 (这就是双重检查的部分),避免另一线程在第一次检查后再做初始化,并且让当前线程获取
锁。
这个模式为什么声名狼藉呢?因为有潜在的条件竞争。未被锁保护的读取操作1没有与其他线程里被锁保护
的写入操作3进行同步,因此就会产生条件竞争,这个条件竞争不仅覆盖指针本身,还会影响到其指向的对
象;即使一个线程知道另一个线程完成对指针进行写入,它可能没有看到新创建的some_resource实例,然
后调用do_something()4后,得到不正确的结果。这个例子是在一种典型的条件竞争——数据竞争,C++标准
中指定为“未定义行为”,这种竞争是可以避免的。阅读第5章时,那里有更多对内存模型的讨论,也包括数据
竞争的构成。(译者注:著名的《C++和双重检查锁定模式(DCLP)的风险》可以作为补充材料供大家参考 英文
版 中文版)
C++标准委员会也认为条件竞争的处理很重要,所以C++标准库提供了 std::once_flag 和 std::call_once 来处
理这种情况。比起锁住互斥量并显式的检查指针,每个线程只需要使用 std::call_once 就可以,
在 std::call_once 的结束时,就能安全的知晓指针已经被其他的线程初始化了。使用 std::call_once 比显式使
用互斥量消耗的资源更少,特别是当初始化完成后。下面的例子展示了与代码3.11中的同样的操作,这里使
用了 std::call_once 。这种情况下,初始化通过调用函数完成,这样的操作使用类中的函数操作符来实现同样
很简单。如同大多数在标准库中的函数一样,或作为函数被调用,或作为参数被传递, std::call_once 可以和
任何函数或可调用对象一起使用。
std::shared_ptr<some_resource> resource_ptr;
std::once_flag resource_flag; // 1
void init_resource()
{
resource_ptr.reset(new some_resource);
}
void foo()
{
std::call_once(resource_flag,init_resource); // 可以完整的进行一次初始化
resource_ptr->do_something();
}


这个例子中, std::once_flag 1和初始化好的数据都是命名空间区域的对象,但 std::call_once() 可仅作为延
迟初始化的类型成员,如同下面的例子一样:
代码3.12 使用 std::call_once 作为类成员的延迟初始化(线程安全)
class X
{
private:
connection_info connection_details;
connection_handle connection;
std::once_flag connection_init_flag;
void open_connection()
{
connection=connection_manager.open(connection_details);
}
public:
X(connection_info const& connection_details_):
connection_details(connection_details_)
{}
void send_data(data_packet const& data) // 1
{
std::call_once(connection_init_flag,&X::open_connection,this); // 2
connection.send_data(data);
}
data_packet receive_data() // 3
{
std::call_once(connection_init_flag,&X::open_connection,this); // 2
return connection.receive_data();
}
};
例子中第一次调用send_data()1或receive_data()3的线程完成初始化过程。使用成员函数
open_connection()去初始化数据,也需要将this指针传进去。和标准库中的函数一样,接受可调用对象,比
如 std::thread 的构造函数和 std::bind() ,通过向 std::call_once() 2传递一个额外的参数来完成这个操作。
值得注意的是, std::mutex 和 std::once_flag 的实例不能拷贝和移动,需要通过显式定义相应的成员函数,对
这些类成员进行操作。
还有一种初始化过程中潜存着条件竞争:其中一个局部变量为static类型,这种变量的在声明后就已经完成初
始化。对于多线程调用的函数,这就意味着这里有条件竞争——抢着去定义这个变量。很多在不支持C++11
标准的编译器上,在实践过程中,这样的条件竞争是确实存在的,因为在多线程中,每个线程都认为他们是
第一个初始化这个变量线程,或一个线程对变量进行初始化,而另外一个线程要使用这个变量时,初始化过
程还没完成。在C++11标准中,这些问题都被解决了:初始化及定义完全在一个线程中发生,并且没有其他
线程可在初始化完成前对其进行处理,条件竞争终止于初始化阶段,这样比在之后再去处理好的多。在只需
要一个全局实例情况下,这里提供一个 std::call_once 的替代方案


class my_class;
my_class& get_my_class_instance()
{
static my_class instance; // 线程安全的初始化过程
return instance;
}
多线程可以安全的调用get_my_class_instance()1函数,不用为数据竞争而担心。
对于很少有更新的数据结构来说,只在初始化时保护数据。大多数情况下,这种数据结构是只读的,并且多
线程对其并发的读取也是很愉快的,不过一旦数据结构需要更新就会产生竞争。
3.3.2 保护不常更新的数据结构
试想为了将域名解析为其相关IP地址,在缓存中的存放了一张DNS入口表。通常,给定DNS数目在很长的时
间内保持不变。虽然,用户访问不同网站时,新的入口可能会被添加到表中,但是这些数据可能在其生命周
期内保持不变。所以定期检查缓存中入口的有效性就变的十分重要。但也需要一次更新,也许这次更新只是
对一些细节做了改动。
虽然更新频度很低,但也有可能发生,并且当缓存多个线程访问时,这个缓存就需要保护更新时状态的状
态,也是为了确保每个线程读到都是有效数据。
没有使用专用数据结构时,这种方式是符合预期的,并为并发更新和读取进行了特别设计(更多的例子在第6
和第7章中介绍)。这样的更新要求线程独占数据结构的访问权,直到更新操作完成。当完成更新时,数据结
构对于并发多线程的访问又会是安全的。使用 std::mutex 来保护数据结构,感觉有些反应过度(因为在没有发
生修改时,它将削减并发读取数据的可能性)。这里需要另一种不同的互斥量,这种互斥量常被称为“读者-作
者锁”,因为其允许两种不同的使用方式:一个“作者”线程独占访问和共享访问,让多个“读者”线程并发访问。
C++17标准库提供了两种非常好的互斥量—— std::shared_mutex 和 std::shared_timed_mutex 。C++14只提供
了 std::shared_timed_mutex ,并且在C++11中并未提供任何互斥量类型。如果还在用支持C++14标准之前的编
译器,可以使用Boost库中的互斥量。 std::shared_mutex 和 std::shared_timed_mutex 的不同点在
于, std::shared_timed_mutex 支持更多的操作方式(参考4.3节), std::shared_mutex 有更高的性能优势,但支持
的操作较少。
第8章中会看到,这种锁的也不能包治百病,其性能依赖于参与其中的处理器数量,同样也与读者和作者线程
的负载有关。为了确保增加复杂度后还能获得性能收益,目标系统上的代码性能就很重要。
比起使用 std::mutex 实例进行同步,不如使用 std::shared_mutex 来做同步。对于更新操作,可以使
用 std::lock_guard<std::shared_mutex> 和 std::unique_lock<std::shared_mutex> 上锁。作为 std::mutex 的替代方
案,与 std::mutex 所做的一样,这就能保证更新线程的独占访问。那些无需修改数据结构的线程,可以使
用 std::shared_lock<std::shared_mutex> 获取访问权。这种RAII类型模板是在C++14中的新特性,这与使
用 std::unique_lock 一样,除了多线程可以同时获取同一个 std::shared_mutex 的共享锁。唯一的限制:当有线
程拥有共享锁时,尝试获取独占锁的线程会被阻塞,直到所有其他线程放弃锁。当任一线程拥有一个独占锁
时,其他线程就无法获得共享锁或独占锁,直到第一个线程放弃其拥有的锁。
如同之前描述的那样,下面的代码清单展示了一个简单的DNS缓存,使用 std::map 持有缓存数据,使
用 std::shared_mutex 进行保护。
代码3.13 使用 std::shared_mutex 对数据结构进行保护


#include <map>
#include <string>
#include <mutex>
#include <shared_mutex>
class dns_entry;
class dns_cache
{
std::map<std::string,dns_entry> entries;
mutable std::shared_mutex entry_mutex;
public:
dns_entry find_entry(std::string const& domain) const
{
std::shared_lock<std::shared_mutex> lk(entry_mutex); // 1
std::map<std::string,dns_entry>::const_iterator const it=
entries.find(domain);
return (it==entries.end())?dns_entry():it->second;
}
void update_or_add_entry(std::string const& domain,
dns_entry const& dns_details)
{
std::lock_guard<std::shared_mutex> lk(entry_mutex); // 2
entries[domain]=dns_details;
}
};
代码3.13中,find_entry()使用 std::shared_lock<> 来保护共享和只读权限1。这就使得多线程可以同时调用
find_entry(),且不会出错。另一方面,update_or_add_entry()使用 std::lock_guard<> 实例,当表格需要更新
时2,为其提供独占访问权限。update_or_add_entry()函数调用时,独占锁会阻止其他线程对数据结构进行
修改,并且阻止线程调用find_entry()。
3.3.3 嵌套锁
线程对已经获取的 std::mutex (已经上锁)再次上锁是错误的,尝试这样做会导致未定义行为。在某些情况下,
一个线程会尝试在释放一个互斥量前多次获取。因此,C++标准库提供了 std::recursive_mutex 类。除了可以
在同一线程的单个实例上多次上锁,其他功能与 std::mutex 相同。其他线程对互斥量上锁前,当前线程必须
释放拥有的所有锁,所以如果你调用lock()三次,也必须调用unlock()三次。正确使
用 std::lock_guard<std::recursive_mutex> 和 std::unique_lock<std::recursive_mutex> 可以帮你处理这些问题。
使用嵌套锁时,要对代码设计进行改动。嵌套锁一般用在可并发访问的类上,所以使用互斥量保护其成员数
据。每个公共成员函数都会对互斥量上锁,然后完成对应的操作后再解锁互斥量。不过,有时成员函数会调
用另一个成员函数,这种情况下,第二个成员函数也会试图锁住互斥量,这就会导致未定义行为的发生。“变
通的”解决方案会将互斥量转为嵌套锁,第二个成员函数就能成功的进行上锁,并且函数能继续执行。
但是这种方式过于草率和不合理,所以不推荐这样的使用方式。特别是,对应类的不变量通常会被破坏。这
意味着,当不变量被破坏时,第二个成员函数还需要继续执行。一个比较好的方式是,从中提取出一个函数
作为类的私有成员,这个私有成员函数不会对互斥量进行上锁(调用前必须获得锁)。然后,需要仔细考虑一


下,这种情况调用新函数时数据的状态。


3.4 本章总结
本章讨论了当线程间的共享数据发生恶性条件竞争时,将会带来多么严重的灾难。还讨论了如何使
用 std::mutex 和如何避免这些问题。虽然C++标准库提供了一些工具来避免这些问题,但互斥量并不是灵丹妙
药,也还有自己的问题(比如:死锁)。还见识了一些用于避免死锁的技术,之后了解了锁的所有权转移,以及
围绕如何选取适当粒度锁产生的问题。最后,在具体情况下讨论了其他数据保护的方案,例
如: std::call_once() 和 std::shared_mutex 。
还有一个方面没有涉及,那就是等待其他线程作为输入的情况。我们的线程安全栈,仅是在栈为空时,抛出
一个异常,所以当一个线程要等待其他线程向栈压入一个值时(这是线程安全栈的主要用途之一),它需要多次
尝试去弹出一个值,当捕获抛出的异常时,再次进行尝试。这种消耗资源的检查,没有任何意义。并且,不
断的检查会影响系统中其他线程的运行,这反而会妨碍程序的运行。我们需要一些方法让一个线程等待其他
线程完成任务,但在等待过程中不占用CPU。第4章中,会去建立一些保护共享数据的工具,还会介绍一些线
程同步的操作机制。第6章中会展示,如何构建更大型的可复用的数据类型。


第4章 同步操作
本章主要内容
带有future的等待
在限定时间内等待
使用同步操作简化代码
上一章中,我们了解了线程间保护共享数据的方法。当然,我们不仅想要保护数据,还想对单独的线程进行
同步。例如,在第一个线程完成前,等待另一个线程执行完成。通常,线程会等待特定事件发生,或者等待
某一条件达成。这可能需要定期检查“任务完成”标识,或将类似的东西放到共享数据中。像这种情况就需要在
线程中进行同步,C++标准库提供了一些工具可用于同步,形式上表现为条件变量(condition variables)和
future。并发技术规范中,为future添加了非常多的操作,并可与新工具锁存器(latches)(轻量级锁资源)和栅栏
(barriers)一起使用。
本章将讨论如何使用条件变量等待事件,介绍future,锁存器和栅栏,以及如何简化同步操作。


4.1 等待事件或条件
假设你正在一辆在夜间运行的火车上,在夜间如何在正确的站点下车呢?有一种方法是整晚都要醒着,每停
一站都能知道,这样就不会错过你要到达的站点,但会很疲倦。另外,可以看一下时间表,估计一下火车到
达目的地的时间,然后在一个稍早的时间点上设置闹铃,然后安心的睡会。这个方法听起来也很不错,也没
有错过你要下车的站点,但是当火车晚点时,就要被过早的叫醒了。当然,闹钟的电池也可能会没电了,并
导致你睡过站。理想的方式是,无论是早或晚,只要当火车到站的时候,有人或其他东西能把你叫醒就好
了。
这和线程有什么关系呢?当一个线程等待另一个线程完成时,可以持续的检查共享数据标志(用于做保护工作
的互斥量),直到另一线程完成工作时对这个标识进行重置。不过,这种方式会消耗线程的执行时间检查标
识,并且当互斥量上锁后,其他线程就没有办法获取锁,就会持续等待。因为对等待线程资源的限制,并且
在任务完成时阻碍对标识的设置。类似于保持清醒状态和列车驾驶员聊了一晚上:驾驶员不得不缓慢驾驶,
因为你分散了他的注意力,所以火车需要更长的时间,才能到站。同样,等待的线程会等待更长的时间,也
会消耗更多的系统资源。
另外,在等待线程在检查间隙,使用 std::this_thread::sleep_for() 进行周期性的间歇(详见4.3节):
bool flag;
std::mutex m;
void wait_for_flag()
{
std::unique_lock<std::mutex> lk(m);
while(!flag)
{
lk.unlock(); // 1 解锁互斥量
std::this_thread::sleep_for(std::chrono::milliseconds(100)); // 2 休眠
100ms
lk.lock(); // 3 再锁互斥量
}
}
循环中,休眠前2函数对互斥量进行解锁1,并且在休眠结束后再对互斥量上锁,所以另外的线程就有机会
获取锁并设置标识。
这个实现就进步很多,当线程休眠时没有浪费执行时间,但很难确定正确的休眠时间。太短的休眠和没有一
样,都会浪费执行时间。太长的休眠时间,可能会让任务等待时间过久。休眠时间过长比较少见,这会影响
到程序的行为,在高节奏的游戏中,就意味着丢帧或错过了一个时间片。
第三个选择(也是优先选择的),使用C++标准库提供的工具去等待事件的发生。通过另一线程触发等待事件的
机制是最基本的唤醒方式(例如:流水线上存在额外的任务时),这种机制就称为“条件变量”。从概念上来说,
条件变量会与多个事件或其他条件相关,并且一个或多个线程会等待条件的达成。当某些线程被终止时,为
了唤醒等待线程(允许等待线程继续执行),终止线程将会向等待着的线程广播“条件达成”的信息。
4.1.1 等待条件达成


C++标准库对条件变量有两套实现: std::condition_variable 和 std::condition_variable_any ,这两个实现都包
含在 <condition_variable> 头文件的声明中。两者都需要与互斥量一起才能工作(互斥量是为了同步),前者仅能
与 std::mutex 一起工作,而后者可以和合适的互斥量一起工作,从而加上了 _any 的后缀。因
为 std::condition_variable_any 更加通用,不过在性能和系统资源的使用方面会有更多的开销,所以通常会
将 std::condition_variable 作为首选类型。当对灵活性有要求时,才会考虑 std::condition_variable_any 。
所以,使用 std::condition_variable 去处理之前提到的情况——当有数据需要处理时,如何唤醒休眠中的线
程?以下代码展示了使用条件变量唤醒线程的方式。
代码4.1 使用 std::condition_variable 处理数据等待
std::mutex mut;
std::queue<data_chunk> data_queue; // 1
std::condition_variable data_cond;
void data_preparation_thread()
{
while(more_data_to_prepare())
{
data_chunk const data=prepare_data();
std::lock_guard<std::mutex> lk(mut);
data_queue.push(data); // 2
data_cond.notify_one(); // 3
}
}
void data_processing_thread()
{
while(true)
{
std::unique_lock<std::mutex> lk(mut); // 4
data_cond.wait(
lk,[]{return !data_queue.empty();}); // 5
data_chunk data=data_queue.front();
data_queue.pop();
lk.unlock(); // 6
process(data);
if(is_last_chunk(data))
break;
}
}
首先,队列中中有两个线程,两个线程之间会对数据进行传递1。数据准备好时,使用 std::lock_guard 锁定
队列,将准备好的数据压入队列2之后,线程会对队列中的数据上锁,并调用 std::condition_variable 的
notify_one()成员函数,对等待的线程(如果有等待线程)进行通知3。
另外的一个线程正在处理数据,线程首先对互斥量上锁(这里使用 std::unique_lock 要比 std::lock_guard 4更加
合适)。之后会调用 std::condition_variable 的成员函数wait(),传递一个锁和一个Lambda表达式(作为等待的
条件5)。Lambda函数是C++11添加的新特性,可以让一个匿名函数作为其他表达式的一部分,并且非常合


适作为标准函数的谓词。例子中,简单的Lambda函数 []{return !data_queue.empty();} 会去检查data_queue是
否为空,当data_queue不为空,就说明数据已经准备好了。附录A的A.5节有Lambda函数更多的信息。
wait()会去检查这些条件(通过Lambda函数),当条件满足(Lambda函数返回true)时返回。如果条件不满足
(Lambda函数返回false),wait()将解锁互斥量,并且将线程(处理数据的线程)置于阻塞或等待状态。当准备数
据的线程调用notify_one()通知条件变量时,处理数据的线程从睡眠中苏醒,重新获取互斥锁,并且再次进行
条件检查。在条件满足的情况下,从wait()返回并继续持有锁。当条件不满足时,线程将对互斥量解锁,并重
新等待。这就是为什么用 std::unique_lock 而不使用 std::lock_guard 的原因——等待中的线程必须在等待期间
解锁互斥量,并对互斥量再次上锁,而 std::lock_guard 没有这么灵活。如果互斥量在线程休眠期间保持锁住
状态,准备数据的线程将无法锁住互斥量,也无法添加数据到队列中。同样,等待线程也永远不会知道条件
何时满足。
代码4.1使用了简单的Lambda函数用于等待5(用于检查队列何时不为空),不过任意的函数和可调用对象都可
以传入wait()。当写好函数做为检查条件时,不一定非要放在一个Lambda表达式中,也可以直接将这个函数
传入wait()。调用wait()的过程中,在互斥量锁定时,可能会去检查条件变量若干次,当提供测试条件的函数
返回true就会立即返回。当等待线程重新获取互斥量并检查条件变量时,并非直接响应另一个线程的通知,就
是所谓的伪唤醒(spurious wakeup)。因为任何伪唤醒的数量和频率都是不确定的,所以不建议使用有副作用
的函数做条件检查。
本质上, std::condition_variable::wait 是“忙碌-等待”的优化。下面用简单的循环实现了一个“忙碌-等待”:
template<typename Predicate>
void minimal_wait(std::unique_lock<std::mutex>& lk, Predicate pred){
while(!pred()){
lk.unlock();
lk.lock();
}
}
为wait()准备一个最小化实现,只需要notify_one()或notify_all()。
std::unique_lock 的灵活性,不仅适用于对wait()的调用,还可以用于待处理的数据6。处理数据可能是耗时
的操作,并且长时间持有锁是个糟糕的主意。
使用队列在多个线程中转移数据(如代码4.1)很常见。做得好的话,同步操作可以在队列内部完成,这样同步
问题和条件竞争出现的概率也会降低。鉴于这些好处,需要从代码4.1中提取出一个通用线程安全的队列。
4.1.2 构建线程安全队列
设计通用队列时,就要花时间想想,哪些操作需要添加到队列实现中去,就如之前在3.2.3节看到的线程安全
的栈。可以看一下C++标准库提供的实现,找找灵感。 std::queue<> 容器的接口展示如下:
代码4.2 std::queue 接口


template <class T, class Container = std::deque<T> >
class queue {
public:
explicit queue(const Container&);
explicit queue(Container&& = Container());
template <class Alloc> explicit queue(const Alloc&);
template <class Alloc> queue(const Container&, const Alloc&);
template <class Alloc> queue(Container&&, const Alloc&);
template <class Alloc> queue(queue&&, const Alloc&);
void swap(queue& q);
bool empty() const;
size_type size() const;
T& front();
const T& front() const;
T& back();
const T& back() const;
void push(const T& x);
void push(T&& x);
void pop();
template <class... Args> void emplace(Args&&... args);
};
忽略构造、赋值以及交换操作,剩下了三组操作:
1. 对整个队列的状态进行查询(empty()和size())
2. 查询在队列中的各个元素(front()和back())
3. 修改队列的操作(push(), pop()和emplace())
和3.2.3中的栈一样,也会遇到接口上的条件竞争。因此,需要将front()和pop()合并成一个函数调用,就像之
前在栈实现时合并top()和pop()一样。与代码4.1不同的是,当队列在多个线程中传递数据时,接收线程通常需
要等待数据的压入。这里提供pop()函数的两个变种:try_pop()和wait_and_pop()。
try_pop() ,尝试从队列中弹出数据,即使没有值可检索,也会直接返回。
wait_and_pop(),将会等待有值可检索的时候才返回。
当使用之前栈的方式来实现队列,接口可能会是下面这样:
代码4.3 线程安全队列的接口


#include <memory> // 为了使用std::shared_ptr
template<typename T>
class threadsafe_queue
{
public:
threadsafe_queue();
threadsafe_queue(const threadsafe_queue&);
threadsafe_queue& operator=(
const threadsafe_queue&) = delete; // 不允许简单的赋值
void push(T new_value);
bool try_pop(T& value); // 1
std::shared_ptr<T> try_pop(); // 2
void wait_and_pop(T& value);
std::shared_ptr<T> wait_and_pop();
bool empty() const;
};
就像之前的栈,裁剪了很多构造函数,并禁止简单赋值。需要提供两个版本的try_pop()和wait_for_pop()。第
一个重载的try_pop()1在引用变量中存储着检索值,可以用来返回队列中值的状态。当检索到一个变量时,
将返回true,否则返回false(详见A.2节)。第二个重载2就不行了,因为它是用来直接返回检索值的,当没有
值可检索时,这个函数返回NULL。
那么问题来了,如何将以上这些和代码4.1相关联呢?从之前的代码中提取push()和wait_and_pop(),如以下
代码所示。
代码4.4 从代码4.1中提取push()和wait_and_pop()


#include <queue>
#include <mutex>
#include <condition_variable>
template<typename T>
class threadsafe_queue
{
private:
std::mutex mut;
std::queue<T> data_queue;
std::condition_variable data_cond;
public:
void push(T new_value)
{
std::lock_guard<std::mutex> lk(mut);
data_queue.push(new_value);
data_cond.notify_one();
}
void wait_and_pop(T& value)
{
std::unique_lock<std::mutex> lk(mut);
data_cond.wait(lk,[this]{return !data_queue.empty();});
value=data_queue.front();
data_queue.pop();
}
};
threadsafe_queue<data_chunk> data_queue; // 1
void data_preparation_thread()
{
while(more_data_to_prepare())
{
data_chunk const data=prepare_data();
data_queue.push(data); // 2
}
}
void data_processing_thread()
{
while(true)
{
data_chunk data;
data_queue.wait_and_pop(data); // 3
process(data);
if(is_last_chunk(data))
break;


}
}
线程队列中有互斥量和条件变量,所以独立的变量就不需要了1,并且push()不需要外部同步2。当然,
wait_and_pop()还要兼顾条件变量的等待3。
另一个wait_and_pop()的重载写起来就很琐碎,剩下的函数就像从代码3.5实现的栈中粘过来一样。
代码4.5 使用条件变量的线程安全队列(完整版)


#include <queue>
#include <memory>
#include <mutex>
#include <condition_variable>
template<typename T>
class threadsafe_queue
{
private:
mutable std::mutex mut; // 1 互斥量必须是可变的
std::queue<T> data_queue;
std::condition_variable data_cond;
public:
threadsafe_queue()
{}
threadsafe_queue(threadsafe_queue const& other)
{
std::lock_guard<std::mutex> lk(other.mut);
data_queue=other.data_queue;
}
void push(T new_value)
{
std::lock_guard<std::mutex> lk(mut);
data_queue.push(new_value);
data_cond.notify_one();
}
void wait_and_pop(T& value)
{
std::unique_lock<std::mutex> lk(mut);
data_cond.wait(lk,[this]{return !data_queue.empty();});
value=data_queue.front();
data_queue.pop();
}
std::shared_ptr<T> wait_and_pop()
{
std::unique_lock<std::mutex> lk(mut);
data_cond.wait(lk,[this]{return !data_queue.empty();});
std::shared_ptr<T> res(std::make_shared<T>(data_queue.front()));
data_queue.pop();
return res;
}
bool try_pop(T& value)
{
std::lock_guard<std::mutex> lk(mut);


if(data_queue.empty())
return false;
value=data_queue.front();
data_queue.pop();
return true;
}
std::shared_ptr<T> try_pop()
{
std::lock_guard<std::mutex> lk(mut);
if(data_queue.empty())
return std::shared_ptr<T>();
std::shared_ptr<T> res(std::make_shared<T>(data_queue.front()));
data_queue.pop();
return res;
}
bool empty() const
{
std::lock_guard<std::mutex> lk(mut);
return data_queue.empty();
}
};
empty()是一个const成员函数,并且传入拷贝构造函数的other形参是一个const引用。因为其他线程可能有非
const引用对象,并调用变种成员函数,所以这里有必要对互斥量上锁。又因为锁住互斥量是个可变操作,所
以互斥量成员必须为mutable1才能在empty()和拷贝构造函数中进行上锁。
条件变量在多个线程等待同一个事件时也很有用。当线程用来分解工作负载,并且只有一个线程可以对通知
做出反应时,与代码4.1中结构完全相同。当数据准备完成时,调用notify_one()将会唤醒一个正在wait()的线
程,检查条件和wait()函数的返回状态(因为仅是向data_queue添加了一个数据项)。 这里不保证线程一定会被
通知到,即使只有一个等待线程收到通知,其他处理线程也有可能因为在处理数据,而忽略了这个通知。
另一种可能是,很多线程等待同一事件。对于通知,都需要做出回应。这会发生在共享数据初始化的时候,
当处理线程使用同一数据时,就要等待数据被初始化,或等待共享数据的更新,比如:周期性初始化(periodic
reinitialization)。这些情况下,线程准备好数据时,就会通过条件变量调用notify_all(),而非调用
notify_one()。顾名思义,这就是全部线程在都去执行wait()(检查他们等待的条件是否满足)的原因。
当条件为true时,等待线程只等待一次,就不会再等待条件变量了,所以尤其是在等待一组可用的数据块时,
一个条件变量并非同步操作最好的选择。
接下来就来了解一下future,对于条件变量的补足。


4.2 使用future
假设你要乘飞机去国外度假,当到达机场办理完各种登机手续后,还需要等待机场广播通知登机。这段时间
内,你可能会在候机室里面找一些事情来打发时间,比如:读书,上网,或者来一杯咖啡。不过,你就在等
待一件事情:机场广播通知登机。
C++标准库将这种事件称为future。当线程需要等待特定事件时,某种程度上来说就需要知道期望的结果。之
后,线程会周期性(较短的周期)的等待或检查事件是否触发(检查信息板),检查期间也会执行其他任务(品尝昂
贵的咖啡)。另外,等待任务期间也可以先执行另外的任务,直到对应的任务触发,而后等待future的状态会
变为就绪状态。future可能是和数据相关(比如,登机口编号),也可能不是。当事件发生时(状态为就绪),这
个future就不能重置了。
C++标准库中有两种future,声明在 <future> 头文件中: unique future( std::future<> )和shared
futures( std::shared_future<> ),与了 std::unique_ptr 和 std::shared_ptr 非常类似。 std::future 只能与指定事
件相关联,而 std::shared_future 就能关联多个事件。后者的实现中,所有实例会在同时变为就绪状态,并且
可以访问与事件相关的数据。这种关联与模板有关,比如 std::unique_ptr 和 std::shared_ptr 的模板参数就是
相关的数据类型。与数据无关处的,可以使用 std::future<void> 与 std::shared_future<void> 的特化模板。虽
然,我倾向于线程通讯,但future对象本身并不提供同步访问。当多个线程需要访问一个独立future对象时,
必须使用互斥量或类似同步机制进行保护。不过,当多个线程对一个 std::shared_future<> 副本进行访问,即
使同一个异步结果,也不需要同步future。
并行技术规范将这两个模板类在 std::experimental 命名空间中进行了扩
展: std::experimental::future<> 和 std::experimental::shared_future<> 。这个命名空间是为了将其与 std 命名
空间中的模板类进行区分,实验命名空间中为这两个模板类添加了更多的功能。尤其是 std::experimental 中的
内容与代码质量无关(我希望这里也会有较高质量的实现),需要强调的是这个命名空间提供的都不是标准类和
函数,这个命名空间中类和函数的语法和语义,很可能与纳入C++标准(也就是 std 命名空间)后有所不同。如
果想要使用这两个试验性的模板类,需要包含 <experimental/future> 头文件。
最简单的事件,就是在后台运行的计算操作。第2章中已经清楚了 std::thread 执行的任务不能有返回值,不
过这个问题能使用future进行解决。
4.2.1 后台任务的返回值
假设有一个需要长时间的运算,需要计算出一个有效值,但并不迫切需要这个值。你可以启动新线程来执行
这个计算,你需要计算的结果,而 std::thread 并不提供直接接收返回值的机制。这里就需要 std::async 函数
模板(也是在头文件 <future> )。
当不着急让任务结果时,可以使用 std::async 启动一个异步任务。与 std::thread 对象等待的方式不
同, std::async 会返回一个 std::future 对象,这个对象持有最终计算出来的结果。当需要这个值时,只需要
调用这个对象的get()成员函数,就会阻塞线程直到future为就绪为止,并返回计算结果。
代码4.6 std::future 从异步任务中获取返回值


#include <future>
#include <iostream>
int find_the_answer_to_ltuae();
void do_other_stuff();
int main()
{
std::future<int> the_answer=std::async(find_the_answer_to_ltuae);
do_other_stuff();
std::cout<<"The answer is "<<the_answer.get()<<std::endl;
}
与 std::thread 方式一样, std::async 允许通过添加额外的调用参数,向函数传递额外的参数。第一个参数是
指向成员函数的指针,第二个参数提供这个函数成员类的具体对象(是通过指针,也可以包装在 std::ref 中),
剩余的参数可作为函数的参数传入。否则,第二个和随后的参数将作为函数的参数,或作为指定可调用对象
的第一个参数。和 std::thread 一样,当参数为右值时,拷贝操作将使用移动的方式转移原始数据,就可以使
用“只移动”类型作为函数对象和参数。
代码4.7 使用 std::async 向函数传递参数


#include <string>
#include <future>
struct X
{
void foo(int,std::string const&);
std::string bar(std::string const&);
};
X x;
auto f1=std::async(&X::foo,&x,42,"hello"); // 调用p->foo(42, "hello"),p是指
向x的指针
auto f2=std::async(&X::bar,x,"goodbye"); // 调用tmpx.bar("goodbye"), tmpx是
x的拷贝副本
struct Y
{
double operator()(double);
};
Y y;
auto f3=std::async(Y(),3.141); // 调用tmpy(3.141),tmpy通过Y的移动构造函数得到
auto f4=std::async(std::ref(y),2.718); // 调用y(2.718)
X baz(X&);
std::async(baz,std::ref(x)); // 调用baz(x)
class move_only
{
public:
move_only();
move_only(move_only&&)
move_only(move_only const&) = delete;
move_only& operator=(move_only&&);
move_only& operator=(move_only const&) = delete;
void operator()();
};
auto f5=std::async(move_only()); // 调用tmp(),tmp是通过
std::move(move_only())构造得到
future的等待取决于 std::async 是否启动一个线程,或是否有任务在进行同步。大多数情况下,也可以在函数
调用之前向 std::async 传递一个额外参数,这个参数的类型是 std::launch ,还可以是 std::launch::defered ,
表明函数调用延迟到wait()或get()函数调用时才执行, std::launch::async 表明函数必须在其所在的独立线程上
执行, std::launch::deferred | std::launch::async 表明实现可以选择这两种方式的一种。最后一个选项是默认
的,当函数调用延迟,就可能不会再运行了。如下所示:


auto f6=std::async(std::launch::async,Y(),1.2); // 在新线程上执行
auto f7=std::async(std::launch::deferred,baz,std::ref(x)); // 在wait()或
get()调用时执行
auto f8=std::async(
std::launch::deferred | std::launch::async,
baz,std::ref(x)); // 实现选择执行方式
auto f9=std::async(baz,std::ref(x));
f7.wait(); // 调用延迟函数
本章的后续小节和第8章中,会再次看到这段程序,使用 std::async 会将算法分割到各个任务中,这样程序就
能并发了。不过,这不是让 std::future 与任务实例相关联的唯一方式,也可以将任务包装
入 std::packaged_task<> 中,或通过编写代码的方式,使用 std::promise<> 模板显式设置值。
与 std::promise<> 相比, std::packaged_task<> 具有更高的抽象,所以我们从“高抽象”模板说起。
4.2.2 future与任务关联
std::packaged_task<> 会将future与函数或可调用对象进行绑定。当调用 std::packaged_task<> 对象时,就会调用
相关函数或可调用对象,当future状态为就绪时,会存储返回值。这可以用在构建线程池(可见第9章)或其他任
务的管理中,比如:在任务所在线程上运行其他任务,或将它们串行运行在一个特殊的后台线程上。当粒度
较大的操作被分解为独立的子任务时,每个子任务都可以包含在 std::packaged_task<> 实例中,之后将实例传
递到任务调度器或线程池中。对任务细节进行抽象,调度器仅处理 std::packaged_task<> 实例,而非处理单独
的函数。
std::packaged_task<> 的模板参数是一个函数签名,比如void()就是一个没有参数也没有返回值的函数,或
int(std::string&, double*)就是有一个非const引用的 std::string 参数和一个指向double类型的指针参数,并且
返回类型是int。构造 std::packaged_task<> 实例时,就必须传入函数或可调用对象。这个函数或可调用的对
象,需要能接收指定的参数和返回(可转换为指定返回类型的)值。类型可以不完全匹配,因为这里类型可以隐
式转换,可以用int类型参数和返回float类型的函数,来构建 std::packaged_task<double(double)> 实例。
函数签名的返回类型可以用来标识从get_future()返回的 std::future<> 的类型,而函数签名的参数列表,可用
来指定packaged_task的函数调用操作符。例如,模板偏特
化 std::packaged_task<std::string(std::vector<char>*,int)> 会在下面的代码中使用到。
代码4.8 std::packaged_task<> 的偏特化
template<>
class packaged_task<std::string(std::vector<char>*,int)>
{
public:
template<typename Callable>
explicit packaged_task(Callable&& f);
std::future<std::string> get_future();
void operator()(std::vector<char>*,int);
};
std::packaged_task 是个可调用对象,可以封装在 std::function 对象中,从而作为线程函数传递
到 std::thread 对象中,或作为可调用对象传递到另一个函数中或直接调用。当 std::packaged_task 作为函数调
用时,实参将由函数调用操作符传递至底层函数,并且返回值作为异步结果存储在 std::future 中,并且可通


过get_future()获取。因此可以用 std::packaged_task 对任务进行打包,并适时的取回future。当异步任务需要
返回值时,可以等待future状态变为“就绪”。
线程间传递任务
很多图形架构需要特定的线程去更新界面,所以当线程对界面更新时,需要发出一条信息给正确的线程,让
相应的线程来做界面更新。 std::packaged_task 提供了这种功能,且不需要发送一条自定义信息给图形界面线
程。
代码4.9 使用 std::packaged_task 执行一个图形界面线程


#include <deque>
#include <mutex>
#include <future>
#include <thread>
#include <utility>
std::mutex m;
std::deque<std::packaged_task<void()> > tasks;
bool gui_shutdown_message_received();
void get_and_process_gui_message();
void gui_thread() // 1
{
while(!gui_shutdown_message_received()) // 2
{
get_and_process_gui_message(); // 3
std::packaged_task<void()> task;
{
std::lock_guard<std::mutex> lk(m);
if(tasks.empty()) // 4
continue;
task=std::move(tasks.front()); // 5
tasks.pop_front();
}
task(); // 6
}
}
std::thread gui_bg_thread(gui_thread);
template<typename Func>
std::future<void> post_task_for_gui_thread(Func f)
{
std::packaged_task<void()> task(f); // 7
std::future<void> res=task.get_future(); // 8
std::lock_guard<std::mutex> lk(m);
tasks.push_back(std::move(task)); // 9
return res; // 10
}
代码十分简单:图形界面线程1循环直到收到一条关闭图形界面的信息后关闭界面2。关闭界面前,进行轮询
界面消息处理3,例如:用户点击和执行在队列中的任务。当队列中没有任务4时,循环将继续。除非能在
队列中提取出一个任务5,释放队列上的锁,并且执行任务6。这里future与任务相关,当任务执行完时,其
状态会置为“就绪”。
将任务传入队列:提供的函数7可以提供一个打包好的任务,通过这个任务8调用get_future()成员函数获取
future对象,并且在任务推入列表9之前,future将返回调用函数10。


例子中使用 std::packaged_task<void()> 创建任务,其中包含了一个无参数无返回值的函数或可调用对象(如果
当这个调用有返回值时,返回值会被丢弃)。这可能是最简单的任务, std::packaged_task 也可以用于复杂的情
况——通过指定不同的函数签名作为模板参数,不仅可以改变其返回类型(因此该类型的数据会存在期望相关
的状态中),也可以改变函数操作符的参数类型。这个例子可以简单的扩展成允许任务运行在图形界面线程
上,并且接受传参,还可以通过 std::future 获取返回值。
这些任务能作为简单的函数调用来表达吗?还有,任务的结果能从很多地方得到吗?这些问题可以使用第三
种方法创建future来解决:使用 std::promise 对值进行显示设置。
4.2.3 使用std::promises
当需要处理很多网络连接时,会使用不同线程尝试连接每个接口,能使网络尽早联通。不幸的是,随着连接
数量的增长,这种方式变的越来越不合适。因为大量的线程会消耗大量的系统资源,还有可能造成线程上下
文频繁切换(当线程数量超出硬件可接受的并发数时),这都会对性能有影响。最极端的例子:线程会将系统资
源消耗殆尽,系统连接网络的能力会变的极差。因此通过少数线程处理网络连接,每个线程同时处理多个连
接,对需要处理大量网络连接的应用而言,这是一种比较普遍的做法。
当线程处理多个连接事件,来自不同的端口连接的数据包基本上以乱序方式进行处理。同样的,数据包也将
以乱序的方式进入队列。很多情况下,一些应用不是等待数据成功的发送,就是等待(新的)指定网络接口数据
的接收成功。
std::promise<T> 提供设定值的方式(类型为T),这个类型会和后面看到的 std::future<T> 对象相关
联。 std::promise/std::future 对提供一种机制:future可以阻塞等待线程,提供数据的线程可以使用promise对
相关值进行设置,并将future的状态置为“就绪”。
可以通过给定的 std::promise 的get_future()成员函数来获取与之相关的 std::future 对象,
与 std::packaged_task 的用法类似。当promise设置完毕(使用set_value()成员函数)时,对应的future状态就变
为“就绪”,并且可用于检索已存储的值。当设置值之前销毁 std::promise ,将会存储一个异常。在4.2.4节中,
会详细描述异常是如何传送到线程的。
代码4.10中是单线程处理多接口的实现,这个例子中,可以使用一对 std::promise<bool>/std::future<bool> 找出
传出成功的数据块,与future相关的只是简单的“成功/失败”标识。对于传入包,与future相关的数据就是数据
包的有效负载。
代码4.10 使用promise解决单线程多连接问题


#include <future>
void process_connections(connection_set& connections)
{
while(!done(connections)) // 1
{
for(connection_iterator // 2
connection=connections.begin(),end=connections.end();
connection!=end;
++connection)
{
if(connection->has_incoming_data()) // 3
{
data_packet data=connection->incoming();
std::promise<payload_type>& p=
connection->get_promise(data.id); // 4
p.set_value(data.payload);
}
if(connection->has_outgoing_data()) // 5
{
outgoing_packet data=
connection->top_of_outgoing_queue();
connection->send(data.payload);
data.promise.set_value(true); // 6
}
}
}
}
process_connections()中(直到done()返回true1为止)每一次循环,都会依次的检查每个连接2,检索是否有
数据3或正在发送已入队的传出数据5。假设输入数据包是具有ID和有效负载的(有实际的数在其中),一个ID
映射到一个 std::promise (可能是在相关容器中进行的依次查找)4,并且值是在包的有效负载中。传出包是在
传出队列中检索,从接口直接发送出去。当发送完成,传出数据相关的promise将置为true,来表明传输成功
6。是否能映射到实际网络协议上,取决于所用协议。
上面的代码不理会异常,一切工作都会很好的执行,但有悖常理。有时候磁盘满载,有时候会找不到东西,
有时候网络会断,还有时候数据库会崩溃。当需要某个操作的结果时,就需要在对应的线程上执行这个操
作,因为代码可以通过异常来报告错误。不过,这会对使用 std::packaged_task 或 std::promise 带来一些不必
要的限制。因此,C++标准库提供了一种在以上情况下清理异常的方法,并且允许将异常存储为相关结果的
一部分。
4.2.4 将异常存与future中
看完下面的代码段,思考一下:当你传递-1到square_root()中时,它将抛出一个异常,并且你想让调用者看
到这个异常:


double square_root(double x)
{
if(x<0)
{
throw std::out_of_range(“x<0”);
}
return sqrt(x);
}
假设调用square_root()函数不是当前线程,
double y=square_root(-1);
将调用改为异步调用:
std::future<double> f=std::async(square_root,-1);
double y=f.get();
当y获得函数调用的结果,线程调用f.get()时,就能再看到异常了。
函数作为 std::async 的一部分时,当调用抛出一个异常时,这个异常就会存储到future中,之后future的状态
置为“就绪”,之后调用get()会抛出已存储的异常(注意:标准级别没有指定重新抛出的这个异常是原始的异常
对象,还是一个拷贝。不同的编译器和库将会在这方面做出不同的选择)。将函数打包入 std::packaged_task 任
务包后,当任务调用时,同样的事情也会发生。打包函数抛出一个异常,这个异常将存储在future中,在get()
调用时会再次抛出。
当然,通过函数的显式调用, std::promise 也能提供同样的功能。当存入的是异常而非数值时,就需要调用
set_exception()成员函数,而非set_value()。这通常是用在一个catch块中,并作为算法的一部分。为了捕获
异常,这里使用异常填充promise:
extern std::promise<double> some_promise;
try
{
some_promise.set_value(calculate_value());
}
catch(...)
{
some_promise.set_exception(std::current_exception());
}
这里使用 std::current_exception() 来检索抛出的异常,可用 std::copy_exception() 作为替代方
案, std::copy_exception() 会直接存储新的异常而不抛出:
some_promise.set_exception(std::copy_exception(std::logic_error("foo ")));
这比使用try/catch块更加清晰,当异常类型已知,就应该优先使用。不是因为代码实现简单,而是给编译器提
供了极大的优化空间。


另一种向future中存储异常的方式,在没有调用promise上的任何设置函数前,或正在调用包装好的任务时,
销毁与 std::promise 或 std::packaged_task 相关的future对象。任何情况下,当future的状态还不是“就绪”时,调
用 std::promise 或 std::packaged_task 的析构函数,将会存储一个与 std::future_errc::broken_promise 错误状态
相关的 std::future_error 异常。通过创建一个future,可以构造一个promise为其提供值或异常,也可以通过
销毁值和异常源,去违背promise。这种情况下,编译器没有在future中存储任何东西,线程可能会永远的等
下去。
现在,例子中都在用 std::future ,不过 std::future 也有局限性。很多线程在等待的时候,只有一个线程能获
取结果。当多个线程等待相同事件的结果时,就需要使用 std::shared_future 来替代 std::future 了。
4.2.5 多个线程的等待
虽然 std::future 可以处理所有在线程间数据转移的同步,但是调用某一特殊 std::future 对象的成员函数,就
会让这个线程的数据和其他线程的数据不同步。多线程在没有额外同步的情况下,访问独立 std::future 对象
时,就会有数据竞争和未定义行为。因为 std::future 独享同步结果,并且通过调用get()函数,一次性的获取
数据,这就让并发访问变的毫无意义。
如果并行代码没办法让多个线程等待同一个事件, std::shared_future 可以帮你解决这个问题。因
为 std::future 是只移动的,所以其所有权可以在不同的实例中互相传递,但只有一个实例可以获得特定的同
步结果,而 std::shared_future 实例是可拷贝的,所以多个对象可以引用同一关联期望值的结果。
每一个 std::shared_future 的独立对象上,成员函数调用返回的结果还是不同步的,所以为了在多个线程访问
一个独立对象时避免数据竞争,必须使用锁来对访问进行保护。优先使用的办法:为了替代只有一个拷贝对
象的情况,可以让每个线程都拥有自己对应的拷贝对象。这样,当每个线程都通过自己拥有
的 std::shared_future 对象获取结果,那么多个线程访问共享同步结果就是安全的。可见图4.1。


图4.1 使用多个 std::shared_future 对象来避免数据竞争
可能会使用 std::shared_future 的场景,例如:实现类似于复杂的电子表格的并行执行,每一个单元格有唯一
终值,这个终值可能由其他单元格中的数据通过公式计算得到。公式计算得到的结果依赖于其他单元格,然
后可以使用 std::shared_future 对象引用第一个单元格的数据。当每个单元格内的所有公式并行执行后,任务
会以期望的方式完成工作。不过,当其中有计算需要依赖其他单元格的值时就会阻塞,直到依赖单元格的数
据准备就绪。这可以让系统在最大程度上使用硬件并发。
std::shared_future 的实例同步 std::future 实例的状态。当 std::future 对象没有与其他对象共享同步状态所
有权,那么所有权必须使用 std::move 将所有权传递到 std::shared_future ,其默认构造函数如下:
std::promise<int> p;
std::future<int> f(p.get_future());
assert(f.valid()); // 1 期望值 f 是合法的
std::shared_future<int> sf(std::move(f));
assert(!f.valid()); // 2 期望值 f 现在是不合法的
assert(sf.valid()); // 3 sf 现在是合法的
期望值f开始是合法的1,因为引用的是promise p的同步状态,但是在转移sf的状态后,f就不合法了2,而sf
就是合法的了3。
如其他可移动对象一样,转移所有权是对右值的隐式操作,所以可以通过 std::promise 对象的成员函数
get_future()的返回值,直接构造一个 std::shared_future 对象,例如:
std::promise<std::string> p;
std::shared_future<std::string> sf(p.get_future()); // 1 隐式转移所有权
转移所有权是隐式的,用右值构造 std::shared_future<> ,得到 std::future<std::string> 类型的实例1。
std::future 的这种特性,可促进 std::shared_future 的使用,容器可以自动的对类型进行推断,从而初始化该
类型的变量(详见附录A,A.6节)。 std::future 有一个share()成员函数,可用来创建新的 std::shared_future
,并且可以直接转移future的所有权。这样也就能保存很多类型,并且使得代码易于修改:


std::promise< std::map< SomeIndexType, SomeDataType, SomeComparator,
SomeAllocator>::iterator> p;
auto sf=p.get_future().share();
这个例子中,sf的类型推导为 std::shared_future<std::map<SomeIndexType, SomeDataType, SomeComparator,
SomeAllocator>::iterator> ,还真的长。当比较器或分配器有所改动,只需要对promise的类型进行修改即可。
future的类型会自动与promise的修改进行匹配。
有时需要限定等待事件的时间,不论是因为时间上有硬性规定(一段指定的代码需要在某段时间内完成),还是
因为在事件没有很快的触发,或是有工作需要特定线程来完成,为了处理这种情况,需要等待函数能对超时
进行指定。


4.3 限时等待
阻塞调用会将线程挂起一段(不确定的)时间,直到相应的事件发生。通常情况下,这样的方式很不错,但是在
一些情况下,需要限定线程等待的时间。可以发送一些类似“我还存活”的信息,无论是对交互式用户,或是其
他进程,亦或当用户放弃等待,也可以按下“取消”键终止等待。
这里介绍两种指定超时方式:一种是“时间段”,另一种是“时间点”。第一种方式,需要指定一段时间(例如,30
毫秒)。第二种方式,就是指定一个时间点(例如,世界标准时间[UTC]17:30:15.045987023,2011年11月30
日)。多数等待函数提供变量,对两种超时方式进行处理。处理持续时间的变量以 _for 作为后缀,处理绝对时
间的变量以 _until 作为后缀。
所以, std::condition_variable 的两个成员函数wait_for()和wait_until()成员函数分别有两个重载,这两个重载
都与wait()成员函数的重载相关——其中一个只是等待信号触发,或超期,亦或伪唤醒,并且醒来时会使用谓
词检查锁,并且只有在校验为true时才会返回(这时条件变量的条件达成),或直接超时。
观察使用超时函数的细节前,我们来检查一下在C++中指定时间的方式,就从“时钟”开始吧!
4.3.1 时钟
对于C++标准库来说,时钟就是时间信息源。并且,时钟是一个类,提供了四种不同的信息:
当前时间
时间类型
时钟节拍
稳定时钟
当前时间可以通过静态成员函数now()从获取。例如, std::chrono::system_clock::now() 会返回系统的当前时
间。特定的时间点可以通过time_point的typedef成员来指定,所以some_clock::now()的类型就是
some_clock::time_point。
时钟节拍被指定为1/x(x在不同硬件上有不同的值)秒,这是由时间周期所决定——一个时钟一秒有25个节拍,
因此一个周期为 std::ratio<1, 25> ,当一个时钟的时钟节拍每2.5秒一次,周期就可以表示为 std::ratio<5,
2> 。当时钟节拍在运行时获取时,可以使用给定的应用程序运行多次,用执行的平均时间求出,其中最短的
时间可能就是时钟节拍,或者是写在手册当中,这就不保证在给定应用中观察到的节拍周期与指定的时钟周
期是否相匹配。
当时钟节拍均匀分布(无论是否与周期匹配),并且不可修改,这种时钟就称为稳定时钟。is_steady静态数据
成员为true时,也表明这个时钟就是稳定的。通常情况下,因为 std::chrono::system_clock 可调,所以是不稳定
的。这可调可能造成首次调用now()返回的时间要早于上次调用now()所返回的时间,这就违反了节拍频率的
均匀分布。稳定闹钟对于计算超时很重要,所以C++标准库提供一个稳定时钟 std::chrono::steady_clock 。
C++标准库提供的其他时钟可表示为 std::chrono::system_clock ,代表了系统时钟的“实际时间”,并且提供了函
数,可将时间点转化为time_t类型的值。 std::chrono::high_resolution_clock 可能是标准库中提供的具有最小节
拍周期(因此具有最高的精度)的时钟。它实际上是typedef的另一种时钟,这些时钟和与时间相关的工具,都
在 <chrono> 库头文件中定义。
我们先看一下时间段是怎么表示的。
4.3.2 时间段


时间部分最简单的就是时间段, std::chrono::duration<> 函数模板能够对时间段进行处理(线程库使用到的所有
C++时间处理工具,都在 std::chrono 命名空间内)。第一个模板参数是一个类型表示(比如,int,long或
double),第二个模板参数是定制部分,表示每一个单元所用秒数。例如,当几分钟的时间要存在short类型中
时,可以写成 std::chrono::duration<short, std::ratio<60, 1>> ,因为60秒是才是1分钟,所以第二个参数写
成 std::ratio<60, 1> 。当需要将毫秒级计数存在double类型中时,可以写成 std::chrono::duration<double,
std::ratio<1, 1000>> ,因为1秒等于1000毫秒。
标准库在 std::chrono 命名空间内为时间段变量提供一系列预定义类型:nanoseconds[纳秒] ,
microseconds[微秒] , milliseconds[毫秒] , seconds[秒] , minutes[分]和hours[时]。比如,你要在一个合适的单
元表示一段超过500年的时延,预定义类型可充分利用了大整型,来表示所要表示的时间类型。当然,这里也
定义了一些国际单位制(SI, [法]le Système international d'unités)分数,可
从 std::atto(10^(-18)) 到 std::exa(10^(18)) (题外话:当你的平台支持128位整型),也可以指定自定义时延类
型。例如: std::duration<double, std::centi> ,就可以使用一个double类型的变量表示1/100。
方便起见,C++14中 std::chrono_literals 命名空间中有许多预定义的后缀操作符用来表示时长。下面的代码
就是使用硬编码的方式赋予变量具体的时长:
using namespace std::chrono_literals;
auto one_day=24h;
auto half_an_hour=30min;
auto max_time_between_messages=30ms;
使用整型字面符时,15ns和 std::chrono::nanoseconds(15) 就是等价的。不过,当使用浮点字面量时,且未指明
表示类型时,数值上会对浮点时长进行适当的缩放。因此,2.5min会被表示为 std::chrono::duration<some
floating-point-type,std::ratio<60,1>> 。如果非常关心所选的浮点类型表示的范围或精度,就需要构造相应的对
象来保证表示范围或精度,而不是去苛求字面值来对范围或精度进行表达。
当不要求截断值的情况下(时转换成秒是没问题,但是秒转换成时就不行)时间段的转换是隐式的,显示转换可
以由 std::chrono::duration_cast<> 来完成。
std::chrono::milliseconds ms(54802);
std::chrono::seconds s=
std::chrono::duration_cast<std::chrono::seconds>(ms);
这里的结果就是截断的,而不是进行了舍入,所以s最后的值为54。
时间值支持四则运算,所以能够对两个时间段进行加减,或者是对一个时间段乘除一个常数(模板的第一个参
数)来获得一个新时间段变量。例如,5*seconds(1)与seconds(5)或minutes(1)-seconds(55)是一样。在时间段
中可以通过count()成员函数获得单位时间的数量。例如, std::chrono::milliseconds(1234).count() 就是1234。
基于时间段的等待可由 std::chrono::duration<> 来完成。例如:等待future状态变为就绪需要35毫秒:
std::future<int> f=std::async(some_task);
if(f.wait_for(std::chrono::milliseconds(35))==std::future_status::ready)
do_something_with(f.get());
等待函数会返回状态值,表示是等待是超时,还是继续等待。等待future时,超时时会返
回 std::future_status::timeout 。当future状态改变,则会返回 std::future_status::ready 。当与future相关的任
务延迟了,则会返回 std::future_status::deferred 。基于时间段的等待使用稳定时钟来计时,所以这里的35毫


秒不受任何影响。当然,系统调度的不确定性和不同操作系统的时钟精度意味着:线程调用和返回的实际时
间间隔可能要比35毫秒长。
现在,来看看“时间点”如何工作。
4.3.3 时间点
时间点可用 std::chrono::time_point<> 来表示,第一个参数用来指定使用的时钟,第二个函数参数用来表示时
间单位(特化的 std::chrono::duration<> )。时间点就是时间戳,而时间戳是时钟的基本属性,不可以直接查
询,其在C++标准中已经指定。通常,UNIX时间戳表示1970年1月1日 00:00。时钟可能共享一个时间戳,或
具有独立的时间戳。当两个时钟共享一个时间戳时,其中一个time_point类型可以与另一个时钟类型中的
time_point相关联。虽然不知道UNIX时间戳的具体值,但可以通过对指定time_point类型使用
time_since_epoch()来获取时间戳,该成员函数会返回一个数值,这个数值是指定时间点与UNIX时间戳的时
间间隔。
例如,指定一个时间点 std::chrono::time_point<std::chrono::system_clock, std::chrono::minutes> ,这就与系统时
钟有关,且实际中的一分钟与系统时钟精度应该不相同(通常差几秒)。
可以通过对 std::chrono::time_point<> 实例进行加/减,来获得一个新的时间点,所
以 std::chrono::hight_resolution_clock::now() + std::chrono::nanoseconds(500) 将得到500纳秒后的时间,这对于
计算绝对时间来说非常方便。
也可以减去一个时间点(二者需要共享同一个时钟),结果是两个时间点的时间差。这对于代码块的计时是很有
用的,例如:
auto start=std::chrono::high_resolution_clock::now();
do_something();
auto stop=std::chrono::high_resolution_clock::now();
std::cout<<”do_something() took “
<<std::chrono::duration<double,std::chrono::seconds>(stop-start).count()
<<” seconds”<<std::endl;
std::chrono::time_point<> 的时钟参数不仅能够指定UNIX时间戳。当等待函数(绝对时间超时)传递时间点时,
时间点参数就可以用来测量时间。当时钟变更时,会产生严重的后果,因为等待轨迹随着时钟的改变而改
变,并且直到调用now()成员函数时,才能返回一个超过超时时间的值。
后缀为 _unitl 的(等待函数的)变量会使用时间点。通常是使用时钟的 ::now() (程序中一个固定的时间点)作为
偏移,虽然时间点与系统时钟有关,可以使用 std::chrono::system_clock::to_time_point() 静态成员函数,对时
间点进行操作。
代码4.11 等待条件变量满足条件——有超时功能